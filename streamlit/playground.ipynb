{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4edd1b9",
   "metadata": {},
   "source": [
    "# Part 4: Parallel Task Execution - Running Multiple SQL Queries Concurrently\n",
    "\n",
    "## The Problem:\n",
    "You have a loop that runs SQL queries one at a time (sequential):\n",
    "```python\n",
    "for sql_query in queries:\n",
    "    result = execute_query(sql_query)  # Slow! Waits for each query\n",
    "```\n",
    "\n",
    "## The Solution:\n",
    "Run multiple queries in parallel (3 at a time) to speed up execution.\n",
    "\n",
    "**What you'll learn:**\n",
    "1. How to use Prefect's `.submit()` for async task execution\n",
    "2. How to limit concurrency (max 3 tasks at once)\n",
    "3. How to collect results from parallel tasks\n",
    "4. Best practices for database connection pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea2c6fd",
   "metadata": {},
   "source": [
    "## Approach 1: Using `.submit()` with Manual Batching\n",
    "\n",
    "This approach runs exactly 3 queries at a time by batching them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0aa570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prefect import flow, task\n",
    "from typing import List, Dict, Any\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Simulate SQL query execution\n",
    "@task(name=\"execute_sql_query\")\n",
    "def execute_sql_query(query_name: str, query: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Simulates executing a SQL query.\n",
    "    In production, this would connect to your database.\n",
    "    \"\"\"\n",
    "    print(f\"ðŸ”µ Starting query: {query_name} at {datetime.now().strftime('%H:%M:%S')}\")\n",
    "    \n",
    "    # Simulate query execution time (1-3 seconds)\n",
    "    import random\n",
    "    execution_time = random.uniform(1, 3)\n",
    "    time.sleep(execution_time)\n",
    "    \n",
    "    # Simulate results\n",
    "    result = {\n",
    "        \"query_name\": query_name,\n",
    "        \"query\": query,\n",
    "        \"row_count\": random.randint(100, 1000),\n",
    "        \"execution_time\": execution_time,\n",
    "        \"completed_at\": datetime.now().strftime('%H:%M:%S')\n",
    "    }\n",
    "    \n",
    "    print(f\"âœ… Completed query: {query_name} ({execution_time:.2f}s)\")\n",
    "    return result\n",
    "\n",
    "\n",
    "@flow(name=\"parallel_sql_batch_approach\")\n",
    "def run_queries_in_batches(queries: List[Dict[str, str]], batch_size: int = 3):\n",
    "    \"\"\"\n",
    "    Run SQL queries in batches of N at a time.\n",
    "    \n",
    "    Args:\n",
    "        queries: List of dicts with 'name' and 'sql' keys\n",
    "        batch_size: Number of queries to run concurrently (default: 3)\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    \n",
    "    # Process queries in batches\n",
    "    for i in range(0, len(queries), batch_size):\n",
    "        batch = queries[i:i + batch_size]\n",
    "        print(f\"\\nðŸ“¦ Processing batch {i//batch_size + 1} ({len(batch)} queries)\")\n",
    "        \n",
    "        # Submit all tasks in this batch\n",
    "        futures = []\n",
    "        for q in batch:\n",
    "            future = execute_sql_query.submit(q[\"name\"], q[\"sql\"])\n",
    "            futures.append(future)\n",
    "        \n",
    "        # Wait for all tasks in this batch to complete\n",
    "        batch_results = [future.result() for future in futures]\n",
    "        all_results.extend(batch_results)\n",
    "        \n",
    "        print(f\"âœ… Batch {i//batch_size + 1} complete\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "\n",
    "# Example: Simulate reading queries from a config file\n",
    "sample_queries = [\n",
    "    {\"name\": \"customer_orders\", \"sql\": \"SELECT * FROM orders WHERE customer_id = 123\"},\n",
    "    {\"name\": \"product_sales\", \"sql\": \"SELECT product_id, SUM(sales) FROM sales GROUP BY product_id\"},\n",
    "    {\"name\": \"inventory_check\", \"sql\": \"SELECT * FROM inventory WHERE stock < 10\"},\n",
    "    {\"name\": \"user_activity\", \"sql\": \"SELECT user_id, COUNT(*) FROM activity GROUP BY user_id\"},\n",
    "    {\"name\": \"revenue_report\", \"sql\": \"SELECT date, SUM(revenue) FROM transactions GROUP BY date\"},\n",
    "    {\"name\": \"top_customers\", \"sql\": \"SELECT customer_id, total_spent FROM customers ORDER BY total_spent DESC LIMIT 100\"},\n",
    "    {\"name\": \"shipping_status\", \"sql\": \"SELECT order_id, status FROM shipments WHERE status = 'pending'\"},\n",
    "    {\"name\": \"returns_analysis\", \"sql\": \"SELECT product_id, COUNT(*) FROM returns GROUP BY product_id\"},\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"APPROACH 1: Manual Batching (3 queries at a time)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total queries: {len(sample_queries)}\")\n",
    "print(f\"Batch size: 3\")\n",
    "print(f\"Expected batches: {len(sample_queries) // 3 + (1 if len(sample_queries) % 3 else 0)}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Run the flow\n",
    "start_time = time.time()\n",
    "results = run_queries_in_batches(sample_queries, batch_size=3)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RESULTS:\")\n",
    "print(\"=\" * 70)\n",
    "for r in results:\n",
    "    print(f\"  â€¢ {r['query_name']}: {r['row_count']} rows in {r['execution_time']:.2f}s\")\n",
    "\n",
    "print(f\"\\nâ±ï¸  Total execution time: {end_time - start_time:.2f} seconds\")\n",
    "print(f\"ðŸ“Š Average time per query: {(end_time - start_time) / len(sample_queries):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec97820",
   "metadata": {},
   "source": [
    "## Approach 2: Using Task Concurrency Limits (Prefect 2.x Feature)\n",
    "\n",
    "This approach uses Prefect's built-in concurrency limiting to automatically control how many tasks run simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782cafb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prefect import flow, task\n",
    "\n",
    "# Define task with concurrency limit\n",
    "@task(\n",
    "    name=\"execute_sql_with_limit\",\n",
    "    tags=[\"database\"],\n",
    "    retries=2,\n",
    "    retry_delay_seconds=10\n",
    ")\n",
    "def execute_sql_with_concurrency_limit(query_name: str, query: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Execute SQL query with automatic concurrency limiting.\n",
    "    Prefect will ensure max 3 of these run at once if configured.\n",
    "    \"\"\"\n",
    "    print(f\"ðŸ”µ Starting query: {query_name} at {datetime.now().strftime('%H:%M:%S')}\")\n",
    "    \n",
    "    import random\n",
    "    execution_time = random.uniform(1, 3)\n",
    "    time.sleep(execution_time)\n",
    "    \n",
    "    result = {\n",
    "        \"query_name\": query_name,\n",
    "        \"query\": query,\n",
    "        \"row_count\": random.randint(100, 1000),\n",
    "        \"execution_time\": execution_time,\n",
    "        \"completed_at\": datetime.now().strftime('%H:%M:%S')\n",
    "    }\n",
    "    \n",
    "    print(f\"âœ… Completed query: {query_name} ({execution_time:.2f}s)\")\n",
    "    return result\n",
    "\n",
    "\n",
    "@flow(name=\"parallel_sql_with_concurrency_limit\")\n",
    "def run_queries_with_limit(queries: List[Dict[str, str]]):\n",
    "    \"\"\"\n",
    "    Submit all queries at once - Prefect controls concurrency automatically.\n",
    "    \n",
    "    Note: In production, set concurrency limit via:\n",
    "      1. Prefect UI: Settings â†’ Concurrency Limits â†’ Create\n",
    "      2. CLI: prefect concurrency-limit create database 3\n",
    "      3. Code: shown in next example\n",
    "    \"\"\"\n",
    "    # Submit ALL queries at once\n",
    "    futures = []\n",
    "    for q in queries:\n",
    "        future = execute_sql_with_concurrency_limit.submit(q[\"name\"], q[\"sql\"])\n",
    "        futures.append(future)\n",
    "    \n",
    "    # Prefect automatically limits concurrent execution to 3\n",
    "    # (if concurrency limit is configured)\n",
    "    results = [future.result() for future in futures]\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"APPROACH 2: Task Concurrency Limits\")\n",
    "print(\"=\" * 70)\n",
    "print(\"This approach submits ALL tasks at once.\")\n",
    "print(\"Prefect's concurrency limiter ensures only 3 run simultaneously.\")\n",
    "print()\n",
    "print(\"âš ï¸  Note: Concurrency limits must be configured on the Prefect server:\")\n",
    "print(\"   prefect concurrency-limit create database 3\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# For this demo, we'll use manual batching since concurrency limits\n",
    "# require server configuration. See next cell for how to set them up.\n",
    "print(\"\\n(Running with manual batching for demo purposes...)\")\n",
    "results = run_queries_in_batches(sample_queries, batch_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0416cdde",
   "metadata": {},
   "source": [
    "## Approach 3: Using Python's asyncio with Semaphore (Most Control)\n",
    "\n",
    "This approach gives you the most control over concurrency using Python's built-in asyncio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89354f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from prefect import flow, task\n",
    "\n",
    "@task(name=\"async_execute_sql\")\n",
    "async def execute_sql_async(query_name: str, query: str, semaphore: asyncio.Semaphore) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Async SQL execution with semaphore-based concurrency control.\n",
    "    The semaphore ensures max 3 queries run at once.\n",
    "    \"\"\"\n",
    "    async with semaphore:  # This blocks if 3 tasks are already running\n",
    "        print(f\"ðŸ”µ Starting query: {query_name} at {datetime.now().strftime('%H:%M:%S')}\")\n",
    "        \n",
    "        import random\n",
    "        execution_time = random.uniform(1, 3)\n",
    "        await asyncio.sleep(execution_time)  # Async sleep\n",
    "        \n",
    "        result = {\n",
    "            \"query_name\": query_name,\n",
    "            \"query\": query,\n",
    "            \"row_count\": random.randint(100, 1000),\n",
    "            \"execution_time\": execution_time,\n",
    "            \"completed_at\": datetime.now().strftime('%H:%M:%S')\n",
    "        }\n",
    "        \n",
    "        print(f\"âœ… Completed query: {query_name} ({execution_time:.2f}s)\")\n",
    "        return result\n",
    "\n",
    "\n",
    "@flow(name=\"parallel_sql_with_semaphore\")\n",
    "async def run_queries_with_semaphore(queries: List[Dict[str, str]], max_concurrent: int = 3):\n",
    "    \"\"\"\n",
    "    Run SQL queries with semaphore-controlled concurrency.\n",
    "    \n",
    "    Args:\n",
    "        queries: List of query configurations\n",
    "        max_concurrent: Maximum number of concurrent queries (default: 3)\n",
    "    \"\"\"\n",
    "    # Create a semaphore that allows max_concurrent tasks at once\n",
    "    semaphore = asyncio.Semaphore(max_concurrent)\n",
    "    \n",
    "    # Create all tasks at once\n",
    "    tasks = [\n",
    "        execute_sql_async(q[\"name\"], q[\"sql\"], semaphore)\n",
    "        for q in queries\n",
    "    ]\n",
    "    \n",
    "    # Run all tasks - semaphore controls concurrency automatically\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"APPROACH 3: asyncio with Semaphore\")\n",
    "print(\"=\" * 70)\n",
    "print(\"This uses Python's asyncio.Semaphore to limit concurrency.\")\n",
    "print(\"Most flexible and works without Prefect server configuration.\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Run the async flow\n",
    "start_time = time.time()\n",
    "results = await run_queries_with_semaphore(sample_queries, max_concurrent=3)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RESULTS:\")\n",
    "print(\"=\" * 70)\n",
    "for r in results:\n",
    "    print(f\"  â€¢ {r['query_name']}: {r['row_count']} rows in {r['execution_time']:.2f}s\")\n",
    "\n",
    "print(f\"\\nâ±ï¸  Total execution time: {end_time - start_time:.2f} seconds\")\n",
    "print(f\"ðŸ“Š Queries completed: {len(results)}\")\n",
    "print(f\"ðŸš€ Concurrency limit: 3 queries at a time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fa1109",
   "metadata": {},
   "source": [
    "## Real-World Example: Loading Queries from SQL Config File\n",
    "\n",
    "Here's a complete example showing how to read queries from a config file and run them in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0db0791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import json\n",
    "from pathlib import Path\n",
    "from prefect import flow, task\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Example 1: Reading from YAML config file\n",
    "sample_yaml_config = \"\"\"\n",
    "queries:\n",
    "  - name: daily_sales\n",
    "    sql: |\n",
    "      SELECT \n",
    "        date,\n",
    "        SUM(amount) as total_sales,\n",
    "        COUNT(*) as transaction_count\n",
    "      FROM sales\n",
    "      WHERE date >= CURRENT_DATE - INTERVAL '7 days'\n",
    "      GROUP BY date\n",
    "    \n",
    "  - name: top_products\n",
    "    sql: |\n",
    "      SELECT \n",
    "        product_id,\n",
    "        product_name,\n",
    "        SUM(quantity) as total_sold\n",
    "      FROM order_items\n",
    "      JOIN products USING (product_id)\n",
    "      WHERE order_date >= CURRENT_DATE - INTERVAL '30 days'\n",
    "      GROUP BY product_id, product_name\n",
    "      ORDER BY total_sold DESC\n",
    "      LIMIT 100\n",
    "    \n",
    "  - name: customer_segments\n",
    "    sql: |\n",
    "      SELECT \n",
    "        customer_segment,\n",
    "        COUNT(DISTINCT customer_id) as customer_count,\n",
    "        AVG(lifetime_value) as avg_ltv\n",
    "      FROM customers\n",
    "      GROUP BY customer_segment\n",
    "\"\"\"\n",
    "\n",
    "# Example 2: Reading from JSON config file\n",
    "sample_json_config = \"\"\"\n",
    "{\n",
    "  \"database\": {\n",
    "    \"host\": \"postgres.company.com\",\n",
    "    \"port\": 5432,\n",
    "    \"database\": \"analytics\"\n",
    "  },\n",
    "  \"queries\": [\n",
    "    {\n",
    "      \"name\": \"inventory_levels\",\n",
    "      \"sql\": \"SELECT warehouse_id, product_id, quantity FROM inventory WHERE quantity < reorder_level\",\n",
    "      \"priority\": \"high\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"pending_orders\",\n",
    "      \"sql\": \"SELECT order_id, customer_id, status, created_at FROM orders WHERE status = 'pending'\",\n",
    "      \"priority\": \"medium\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@task(name=\"load_queries_from_config\")\n",
    "def load_queries_from_file(config_path: str, format: str = \"yaml\") -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Load SQL queries from a configuration file.\n",
    "    \n",
    "    Args:\n",
    "        config_path: Path to the config file\n",
    "        format: File format ('yaml' or 'json')\n",
    "    \n",
    "    Returns:\n",
    "        List of query dictionaries with 'name' and 'sql' keys\n",
    "    \"\"\"\n",
    "    # In this demo, we'll use the sample configs\n",
    "    # In production, you'd read from actual files\n",
    "    \n",
    "    if format == \"yaml\":\n",
    "        config = yaml.safe_load(sample_yaml_config)\n",
    "    else:\n",
    "        config = json.loads(sample_json_config)\n",
    "    \n",
    "    queries = config.get(\"queries\", [])\n",
    "    \n",
    "    print(f\"ðŸ“„ Loaded {len(queries)} queries from config file\")\n",
    "    for q in queries:\n",
    "        print(f\"   â€¢ {q['name']}\")\n",
    "    \n",
    "    return queries\n",
    "\n",
    "\n",
    "@task(name=\"execute_database_query\")\n",
    "def execute_database_query(query_name: str, query: str, db_config: Dict[str, Any] = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Execute a SQL query against a database.\n",
    "    \n",
    "    In production, this would:\n",
    "    1. Get connection from pool\n",
    "    2. Execute query\n",
    "    3. Fetch results\n",
    "    4. Return connection to pool\n",
    "    \"\"\"\n",
    "    print(f\"ðŸ”µ Executing: {query_name} at {datetime.now().strftime('%H:%M:%S')}\")\n",
    "    \n",
    "    # Simulate database query execution\n",
    "    import random\n",
    "    execution_time = random.uniform(1, 3)\n",
    "    time.sleep(execution_time)\n",
    "    \n",
    "    # Simulate results\n",
    "    result = {\n",
    "        \"query_name\": query_name,\n",
    "        \"success\": True,\n",
    "        \"row_count\": random.randint(50, 500),\n",
    "        \"execution_time\": execution_time,\n",
    "        \"data\": f\"[Sample data from {query_name}]\"\n",
    "    }\n",
    "    \n",
    "    print(f\"âœ… Completed: {query_name} - {result['row_count']} rows ({execution_time:.2f}s)\")\n",
    "    return result\n",
    "\n",
    "\n",
    "@flow(name=\"production_etl_pipeline\")\n",
    "def run_etl_pipeline(config_path: str = \"queries.yaml\", max_concurrent: int = 3):\n",
    "    \"\"\"\n",
    "    Production-ready ETL pipeline that:\n",
    "    1. Loads queries from config file\n",
    "    2. Executes them in parallel (max 3 at a time)\n",
    "    3. Collects and returns results\n",
    "    \n",
    "    Args:\n",
    "        config_path: Path to query config file\n",
    "        max_concurrent: Maximum concurrent queries\n",
    "    \"\"\"\n",
    "    # Step 1: Load queries from config\n",
    "    queries = load_queries_from_file(config_path, format=\"yaml\")\n",
    "    \n",
    "    # Step 2: Run queries in batches\n",
    "    all_results = []\n",
    "    \n",
    "    for i in range(0, len(queries), max_concurrent):\n",
    "        batch = queries[i:i + max_concurrent]\n",
    "        batch_num = i // max_concurrent + 1\n",
    "        \n",
    "        print(f\"\\nðŸ“¦ Batch {batch_num}: Running {len(batch)} queries\")\n",
    "        \n",
    "        # Submit batch\n",
    "        futures = [\n",
    "            execute_database_query.submit(q[\"name\"], q[\"sql\"])\n",
    "            for q in batch\n",
    "        ]\n",
    "        \n",
    "        # Wait for completion\n",
    "        batch_results = [f.result() for f in futures]\n",
    "        all_results.extend(batch_results)\n",
    "    \n",
    "    # Step 3: Summary\n",
    "    successful = sum(1 for r in all_results if r[\"success\"])\n",
    "    total_rows = sum(r[\"row_count\"] for r in all_results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ETL PIPELINE SUMMARY:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"âœ… Successful queries: {successful}/{len(all_results)}\")\n",
    "    print(f\"ðŸ“Š Total rows processed: {total_rows:,}\")\n",
    "    print(f\"ðŸš€ Concurrency: {max_concurrent} queries at a time\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "\n",
    "# Run the production pipeline\n",
    "print(\"=\" * 70)\n",
    "print(\"PRODUCTION EXAMPLE: ETL Pipeline with Config File\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "start_time = time.time()\n",
    "results = run_etl_pipeline(config_path=\"queries.yaml\", max_concurrent=3)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"\\nâ±ï¸  Total pipeline time: {end_time - start_time:.2f} seconds\")\n",
    "print(f\"ðŸ“ˆ Average query time: {(end_time - start_time) / len(results):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9280d8a",
   "metadata": {},
   "source": [
    "## Production Best Practices: Database Connection Pooling\n",
    "\n",
    "When running parallel SQL queries, use connection pooling to avoid overwhelming your database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334c5c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production example with connection pooling\n",
    "production_example = \"\"\"\n",
    "# ============================================================================\n",
    "# PRODUCTION CODE: Parallel SQL Queries with Connection Pooling\n",
    "# ============================================================================\n",
    "\n",
    "from prefect import flow, task\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.pool import QueuePool\n",
    "from typing import List, Dict, Any\n",
    "import pandas as pd\n",
    "\n",
    "# Global connection pool (created once, reused across tasks)\n",
    "# This is crucial for parallel execution!\n",
    "engine = create_engine(\n",
    "    \"postgresql://user:password@host:5432/database\",\n",
    "    poolclass=QueuePool,\n",
    "    pool_size=5,          # Base pool size\n",
    "    max_overflow=10,      # Can grow to 15 total connections\n",
    "    pool_pre_ping=True,   # Test connections before using\n",
    "    echo=False\n",
    ")\n",
    "\n",
    "\n",
    "@task(\n",
    "    name=\"execute_query_with_pool\",\n",
    "    retries=3,\n",
    "    retry_delay_seconds=10\n",
    ")\n",
    "def execute_query_with_pool(query_name: str, sql: str) -> pd.DataFrame:\n",
    "    \\\"\\\"\\\"\n",
    "    Execute SQL query using connection from pool.\n",
    "    \n",
    "    Connection pooling ensures:\n",
    "    - We don't create too many database connections\n",
    "    - Connections are reused efficiently\n",
    "    - Failed connections are recycled\n",
    "    \\\"\\\"\\\"\n",
    "    try:\n",
    "        # Get connection from pool\n",
    "        with engine.connect() as conn:\n",
    "            # Execute query\n",
    "            result = pd.read_sql(text(sql), conn)\n",
    "            \n",
    "            print(f\"âœ… {query_name}: {len(result)} rows\")\n",
    "            return result\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {query_name} failed: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "@flow(name=\"parallel_queries_production\")\n",
    "def run_parallel_queries(queries: List[Dict[str, str]], batch_size: int = 3):\n",
    "    \\\"\\\"\\\"\n",
    "    Run queries in parallel with connection pooling.\n",
    "    \n",
    "    The pool automatically limits concurrent connections,\n",
    "    so we can safely submit more tasks than we have connections.\n",
    "    \\\"\\\"\\\"\n",
    "    all_results = {}\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, len(queries), batch_size):\n",
    "        batch = queries[i:i + batch_size]\n",
    "        \n",
    "        # Submit batch (pool handles connection limiting)\n",
    "        futures = [\n",
    "            execute_query_with_pool.submit(q[\"name\"], q[\"sql\"])\n",
    "            for q in batch\n",
    "        ]\n",
    "        \n",
    "        # Collect results\n",
    "        for q, future in zip(batch, futures):\n",
    "            all_results[q[\"name\"]] = future.result()\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EXAMPLE USAGE\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load queries from config\n",
    "    import yaml\n",
    "    \n",
    "    with open(\"queries.yaml\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    # Run with 3 concurrent queries\n",
    "    results = run_parallel_queries(\n",
    "        queries=config[\"queries\"],\n",
    "        batch_size=3\n",
    "    )\n",
    "    \n",
    "    # Save results\n",
    "    for name, df in results.items():\n",
    "        df.to_parquet(f\"output/{name}.parquet\")\n",
    "        print(f\"Saved {name}: {len(df)} rows\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# KEY BENEFITS:\n",
    "# ============================================================================\n",
    "# \n",
    "# âœ… Connection Pooling:\n",
    "#    - Reuses database connections efficiently\n",
    "#    - Prevents connection exhaustion\n",
    "#    - Handles connection failures gracefully\n",
    "#\n",
    "# âœ… Parallel Execution:\n",
    "#    - Runs 3 queries at once\n",
    "#    - Reduces total pipeline time\n",
    "#    - Better resource utilization\n",
    "#\n",
    "# âœ… Error Handling:\n",
    "#    - Automatic retries (3 attempts)\n",
    "#    - Doesn't fail entire pipeline if one query fails\n",
    "#    - Detailed error logging\n",
    "#\n",
    "# âœ… Monitoring:\n",
    "#    - All queries tracked in Prefect UI\n",
    "#    - Can see which queries are slow\n",
    "#    - Full execution history\n",
    "#\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "\n",
    "print(production_example)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CONNECTION POOL CONFIGURATION GUIDE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "pooling_guide = \"\"\"\n",
    "When configuring connection pools for parallel queries:\n",
    "\n",
    "1. pool_size (base connections):\n",
    "   - Set to your typical concurrency level\n",
    "   - Example: If running 3 queries at a time â†’ pool_size=3 to 5\n",
    "\n",
    "2. max_overflow (extra connections):\n",
    "   - Handles bursts when needed\n",
    "   - Example: max_overflow=5 allows up to 10 total (if pool_size=5)\n",
    "\n",
    "3. pool_pre_ping (connection testing):\n",
    "   - Always set to True for production\n",
    "   - Tests connections before use to avoid stale connection errors\n",
    "\n",
    "4. Database limits:\n",
    "   - Check your database's max_connections setting\n",
    "   - Leave headroom for other applications\n",
    "   - Example: DB max=100, use pool_size=5 + max_overflow=10 per worker\n",
    "\n",
    "5. Multiple workers:\n",
    "   - If running 3 workers, each needs its own pool\n",
    "   - Total connections = workers Ã— (pool_size + max_overflow)\n",
    "   - Example: 3 workers Ã— 15 max = 45 total connections\n",
    "\n",
    "RECOMMENDED SETTINGS FOR PARALLEL QUERIES:\n",
    "\n",
    "# For 3 concurrent queries per worker:\n",
    "engine = create_engine(\n",
    "    connection_string,\n",
    "    poolclass=QueuePool,\n",
    "    pool_size=3,              # Match concurrency\n",
    "    max_overflow=2,           # Allow bursts\n",
    "    pool_pre_ping=True,       # Test connections\n",
    "    pool_recycle=3600,        # Recycle connections every hour\n",
    "    echo=False                # Set True for debugging\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "print(pooling_guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fc1f30",
   "metadata": {},
   "source": [
    "## Summary: Comparison of Approaches\n",
    "\n",
    "Let's compare the three approaches and help you choose the right one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb58e94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create comparison table\n",
    "comparison_data = {\n",
    "    \"Approach\": [\n",
    "        \"1. Manual Batching\\n(.submit() + batching)\",\n",
    "        \"2. Concurrency Limits\\n(Prefect server config)\",\n",
    "        \"3. asyncio Semaphore\\n(Python native)\"\n",
    "    ],\n",
    "    \"Pros\": [\n",
    "        \"â€¢ Simple to understand\\nâ€¢ No server config needed\\nâ€¢ Works immediately\\nâ€¢ Full control over batching\",\n",
    "        \"â€¢ Clean code\\nâ€¢ Centralized control\\nâ€¢ Server enforces limits\\nâ€¢ Works across deployments\",\n",
    "        \"â€¢ Most flexible\\nâ€¢ No dependencies\\nâ€¢ Great for async code\\nâ€¢ Fine-grained control\"\n",
    "    ],\n",
    "    \"Cons\": [\n",
    "        \"â€¢ Manual batch logic\\nâ€¢ More verbose code\\nâ€¢ Harder to change limit\",\n",
    "        \"â€¢ Requires server setup\\nâ€¢ Must configure limits first\\nâ€¢ Less obvious in code\",\n",
    "        \"â€¢ Requires async/await\\nâ€¢ More complex syntax\\nâ€¢ Need to understand asyncio\"\n",
    "    ],\n",
    "    \"Best For\": [\n",
    "        \"â€¢ Quick prototypes\\nâ€¢ Simple pipelines\\nâ€¢ Learning Prefect\\nâ€¢ No server access\",\n",
    "        \"â€¢ Production systems\\nâ€¢ Multiple deployments\\nâ€¢ Team workflows\\nâ€¢ Centralized governance\",\n",
    "        \"â€¢ Complex async flows\\nâ€¢ Maximum performance\\nâ€¢ I/O-bound operations\\nâ€¢ Advanced users\"\n",
    "    ],\n",
    "    \"Recommended?\": [\n",
    "        \"âœ… Start here\",\n",
    "        \"ðŸš€ Production\",\n",
    "        \"âš¡ Advanced\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"COMPARISON OF PARALLEL EXECUTION APPROACHES\")\n",
    "print(\"=\" * 100)\n",
    "print()\n",
    "for idx, row in df.iterrows():\n",
    "    print(f\"{'='*100}\")\n",
    "    print(f\"{row['Approach']} - {row['Recommended?']}\")\n",
    "    print(f\"{'='*100}\")\n",
    "    print(f\"\\nâœ… PROS:\\n{row['Pros']}\")\n",
    "    print(f\"\\nâŒ CONS:\\n{row['Cons']}\")\n",
    "    print(f\"\\nðŸŽ¯ BEST FOR:\\n{row['Best For']}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"RECOMMENDATIONS:\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "recommendations = \"\"\"\n",
    "ðŸ“ For Your Use Case (SQL queries from config file):\n",
    "\n",
    "1ï¸âƒ£ START WITH APPROACH 1 (Manual Batching):\n",
    "   - Easy to implement right now\n",
    "   - No extra setup required\n",
    "   - Perfect for getting started\n",
    "   - Code example provided above\n",
    "\n",
    "2ï¸âƒ£ MOVE TO APPROACH 2 (Concurrency Limits) when:\n",
    "   - You have Prefect server running\n",
    "   - Multiple people on the team\n",
    "   - Want centralized control\n",
    "   - Running in production\n",
    "\n",
    "3ï¸âƒ£ CONSIDER APPROACH 3 (Semaphore) if:\n",
    "   - You need maximum performance\n",
    "   - Already using async/await\n",
    "   - Have complex async operations\n",
    "   - Comfortable with asyncio\n",
    "\n",
    "\n",
    "ðŸ† RECOMMENDED IMPLEMENTATION FOR YOU:\n",
    "\n",
    "from prefect import flow, task\n",
    "from typing import List, Dict\n",
    "import yaml\n",
    "\n",
    "@task(name=\"execute_sql\", retries=2)\n",
    "def execute_sql(query_name: str, sql: str) -> dict:\n",
    "    # Your database query logic here\n",
    "    with db_connection_pool.connect() as conn:\n",
    "        result = pd.read_sql(sql, conn)\n",
    "        return {\"name\": query_name, \"rows\": len(result), \"data\": result}\n",
    "\n",
    "@flow(name=\"parallel_etl\")\n",
    "def run_etl(config_file: str, batch_size: int = 3):\n",
    "    # Load queries from config\n",
    "    with open(config_file) as f:\n",
    "        queries = yaml.safe_load(f)[\"queries\"]\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    # Process in batches of 3\n",
    "    for i in range(0, len(queries), batch_size):\n",
    "        batch = queries[i:i + batch_size]\n",
    "        \n",
    "        # Submit all queries in this batch\n",
    "        futures = [execute_sql.submit(q[\"name\"], q[\"sql\"]) for q in batch]\n",
    "        \n",
    "        # Wait for batch to complete\n",
    "        batch_results = [f.result() for f in futures]\n",
    "        all_results.extend(batch_results)\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Usage:\n",
    "results = run_etl(\"queries.yaml\", batch_size=3)\n",
    "\"\"\"\n",
    "\n",
    "print(recommendations)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "perf_example = \"\"\"\n",
    "Example: 10 queries, each takes 2 seconds\n",
    "\n",
    "Sequential (old way):\n",
    "  â€¢ Query 1: 2s\n",
    "  â€¢ Query 2: 2s\n",
    "  â€¢ ... (8 more)\n",
    "  â€¢ Query 10: 2s\n",
    "  TOTAL: 20 seconds âŒ\n",
    "\n",
    "Parallel with batch_size=3 (new way):\n",
    "  â€¢ Batch 1 (queries 1-3): 2s (all run together)\n",
    "  â€¢ Batch 2 (queries 4-6): 2s (all run together)\n",
    "  â€¢ Batch 3 (queries 7-9): 2s (all run together)\n",
    "  â€¢ Batch 4 (query 10): 2s\n",
    "  TOTAL: 8 seconds âœ… (2.5x faster!)\n",
    "\n",
    "Parallel with batch_size=5:\n",
    "  â€¢ Batch 1 (queries 1-5): 2s\n",
    "  â€¢ Batch 2 (queries 6-10): 2s\n",
    "  TOTAL: 4 seconds âœ… (5x faster!)\n",
    "\n",
    "âš ï¸  BUT: More concurrency = more database load\n",
    "    Always check your database can handle it!\n",
    "\"\"\"\n",
    "\n",
    "print(perf_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c3cc49",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Quick Start: Your Exact Use Case\n",
    "\n",
    "Here's the complete code you can copy and adapt for your SQL config file scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb797d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_solution = \"\"\"\n",
    "# ============================================================================\n",
    "# COMPLETE SOLUTION: Parallel SQL Queries from Config File\n",
    "# ============================================================================\n",
    "# \n",
    "# This is ready-to-use code that you can adapt to your project.\n",
    "# It runs 3 SQL queries at a time instead of 1.\n",
    "#\n",
    "\n",
    "from prefect import flow, task\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.pool import QueuePool\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: Database Connection Pool Setup\n",
    "# ============================================================================\n",
    "\n",
    "def get_db_engine():\n",
    "    \\\"\\\"\\\"\n",
    "    Create database engine with connection pooling.\n",
    "    Modify the connection string for your database.\n",
    "    \\\"\\\"\\\"\n",
    "    return create_engine(\n",
    "        # Replace with your database connection string:\n",
    "        \"postgresql://user:password@host:5432/database\",\n",
    "        # Connection pool settings:\n",
    "        poolclass=QueuePool,\n",
    "        pool_size=3,           # Match your concurrency level\n",
    "        max_overflow=2,        # Allow some burst capacity\n",
    "        pool_pre_ping=True,    # Test connections before use\n",
    "        pool_recycle=3600,     # Recycle connections hourly\n",
    "        echo=False             # Set True for SQL logging\n",
    "    )\n",
    "\n",
    "\n",
    "# Global engine (created once, reused)\n",
    "engine = get_db_engine()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: Task to Execute Individual Queries\n",
    "# ============================================================================\n",
    "\n",
    "@task(\n",
    "    name=\"execute_single_query\",\n",
    "    retries=2,\n",
    "    retry_delay_seconds=10,\n",
    "    tags=[\"database\", \"sql\"]\n",
    ")\n",
    "def execute_query(query_name: str, sql: str) -> Dict[str, Any]:\n",
    "    \\\"\\\"\\\"\n",
    "    Execute a single SQL query and return results.\n",
    "    \n",
    "    Args:\n",
    "        query_name: Name/identifier for the query\n",
    "        sql: SQL query string to execute\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with query results and metadata\n",
    "    \\\"\\\"\\\"\n",
    "    try:\n",
    "        # Get connection from pool\n",
    "        with engine.connect() as conn:\n",
    "            # Execute query and load into DataFrame\n",
    "            df = pd.read_sql(text(sql), conn)\n",
    "            \n",
    "            result = {\n",
    "                \"name\": query_name,\n",
    "                \"success\": True,\n",
    "                \"row_count\": len(df),\n",
    "                \"columns\": list(df.columns),\n",
    "                \"data\": df,\n",
    "                \"error\": None\n",
    "            }\n",
    "            \n",
    "            print(f\"âœ… {query_name}: Retrieved {len(df)} rows\")\n",
    "            return result\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {query_name} failed: {str(e)}\")\n",
    "        return {\n",
    "            \"name\": query_name,\n",
    "            \"success\": False,\n",
    "            \"row_count\": 0,\n",
    "            \"columns\": [],\n",
    "            \"data\": None,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: Load Queries from Config File\n",
    "# ============================================================================\n",
    "\n",
    "@task(name=\"load_query_config\")\n",
    "def load_queries(config_path: str) -> List[Dict[str, str]]:\n",
    "    \\\"\\\"\\\"\n",
    "    Load SQL queries from YAML or JSON config file.\n",
    "    \n",
    "    Expected format (YAML):\n",
    "    queries:\n",
    "      - name: query1\n",
    "        sql: SELECT * FROM table1\n",
    "      - name: query2\n",
    "        sql: SELECT * FROM table2\n",
    "    \n",
    "    Args:\n",
    "        config_path: Path to config file\n",
    "    \n",
    "    Returns:\n",
    "        List of query dictionaries\n",
    "    \\\"\\\"\\\"\n",
    "    config_file = Path(config_path)\n",
    "    \n",
    "    if config_file.suffix in ['.yaml', '.yml']:\n",
    "        with open(config_file) as f:\n",
    "            config = yaml.safe_load(f)\n",
    "    elif config_file.suffix == '.json':\n",
    "        import json\n",
    "        with open(config_file) as f:\n",
    "            config = json.load(f)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {config_file.suffix}\")\n",
    "    \n",
    "    queries = config.get(\"queries\", [])\n",
    "    print(f\"ðŸ“„ Loaded {len(queries)} queries from {config_path}\")\n",
    "    \n",
    "    return queries\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: Main Flow - Run Queries in Parallel (3 at a time)\n",
    "# ============================================================================\n",
    "\n",
    "@flow(name=\"parallel_sql_pipeline\")\n",
    "def run_parallel_queries(\n",
    "    config_path: str,\n",
    "    batch_size: int = 3,\n",
    "    output_dir: str = \"output\"\n",
    ") -> Dict[str, Any]:\n",
    "    \\\"\\\"\\\"\n",
    "    Main flow that executes SQL queries in parallel.\n",
    "    \n",
    "    Args:\n",
    "        config_path: Path to query configuration file\n",
    "        batch_size: Number of queries to run concurrently (default: 3)\n",
    "        output_dir: Directory to save results (optional)\n",
    "    \n",
    "    Returns:\n",
    "        Summary of execution results\n",
    "    \\\"\\\"\\\"\n",
    "    # Load queries from config file\n",
    "    queries = load_queries(config_path)\n",
    "    \n",
    "    if not queries:\n",
    "        print(\"âš ï¸  No queries found in config file\")\n",
    "        return {\"success\": False, \"results\": []}\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    # Process queries in batches\n",
    "    total_batches = (len(queries) + batch_size - 1) // batch_size\n",
    "    \n",
    "    for i in range(0, len(queries), batch_size):\n",
    "        batch = queries[i:i + batch_size]\n",
    "        batch_num = i // batch_size + 1\n",
    "        \n",
    "        print(f\"\\\\n{'='*70}\")\n",
    "        print(f\"ðŸ“¦ Batch {batch_num}/{total_batches}: Running {len(batch)} queries\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Submit all queries in this batch (runs in parallel)\n",
    "        futures = [\n",
    "            execute_query.submit(q[\"name\"], q[\"sql\"])\n",
    "            for q in batch\n",
    "        ]\n",
    "        \n",
    "        # Wait for all queries in batch to complete\n",
    "        batch_results = [future.result() for future in futures]\n",
    "        all_results.extend(batch_results)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Optional: Save results to files\n",
    "    # ========================================================================\n",
    "    if output_dir:\n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(exist_ok=True)\n",
    "        \n",
    "        for result in all_results:\n",
    "            if result[\"success\"] and result[\"data\"] is not None:\n",
    "                filename = output_path / f\"{result['name']}.parquet\"\n",
    "                result[\"data\"].to_parquet(filename)\n",
    "                print(f\"ðŸ’¾ Saved {result['name']} to {filename}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Summary\n",
    "    # ========================================================================\n",
    "    successful = sum(1 for r in all_results if r[\"success\"])\n",
    "    failed = len(all_results) - successful\n",
    "    total_rows = sum(r[\"row_count\"] for r in all_results if r[\"success\"])\n",
    "    \n",
    "    print(f\"\\\\n{'='*70}\")\n",
    "    print(\"ðŸ“Š PIPELINE SUMMARY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"âœ… Successful queries: {successful}/{len(all_results)}\")\n",
    "    print(f\"âŒ Failed queries: {failed}\")\n",
    "    print(f\"ðŸ“ˆ Total rows retrieved: {total_rows:,}\")\n",
    "    print(f\"ðŸš€ Concurrency: {batch_size} queries at a time\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    return {\n",
    "        \"success\": failed == 0,\n",
    "        \"total_queries\": len(all_results),\n",
    "        \"successful\": successful,\n",
    "        \"failed\": failed,\n",
    "        \"total_rows\": total_rows,\n",
    "        \"results\": all_results\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# USAGE EXAMPLE\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the pipeline\n",
    "    summary = run_parallel_queries(\n",
    "        config_path=\"queries.yaml\",\n",
    "        batch_size=3,\n",
    "        output_dir=\"query_results\"\n",
    "    )\n",
    "    \n",
    "    # Check results\n",
    "    if summary[\"success\"]:\n",
    "        print(f\"\\\\nðŸŽ‰ All {summary['successful']} queries completed successfully!\")\n",
    "    else:\n",
    "        print(f\"\\\\nâš ï¸  {summary['failed']} queries failed. Check logs for details.\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EXAMPLE CONFIG FILE (queries.yaml)\n",
    "# ============================================================================\n",
    "#\n",
    "# queries:\n",
    "#   - name: daily_sales\n",
    "#     sql: |\n",
    "#       SELECT \n",
    "#         date,\n",
    "#         SUM(amount) as total_sales\n",
    "#       FROM sales\n",
    "#       WHERE date >= CURRENT_DATE - INTERVAL '7 days'\n",
    "#       GROUP BY date\n",
    "#   \n",
    "#   - name: top_products\n",
    "#     sql: |\n",
    "#       SELECT \n",
    "#         product_id,\n",
    "#         COUNT(*) as order_count\n",
    "#       FROM orders\n",
    "#       GROUP BY product_id\n",
    "#       ORDER BY order_count DESC\n",
    "#       LIMIT 100\n",
    "#\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "\n",
    "print(complete_solution)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"ðŸš€ TO USE THIS IN YOUR PROJECT:\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "usage_steps = \"\"\"\n",
    "1. Create queries.yaml with your SQL queries:\n",
    "   \n",
    "   queries:\n",
    "     - name: my_query_1\n",
    "       sql: SELECT * FROM table1\n",
    "     - name: my_query_2\n",
    "       sql: SELECT * FROM table2\n",
    "     # ... add more queries\n",
    "\n",
    "2. Update database connection string in get_db_engine():\n",
    "   \n",
    "   \"postgresql://user:password@host:5432/database\"\n",
    "   # Or use environment variables:\n",
    "   import os\n",
    "   os.getenv(\"DATABASE_URL\")\n",
    "\n",
    "3. Run the flow:\n",
    "   \n",
    "   python your_script.py\n",
    "   \n",
    "   # Or deploy it:\n",
    "   prefect deploy your_script.py:run_parallel_queries\n",
    "\n",
    "4. Monitor in Prefect UI:\n",
    "   \n",
    "   http://localhost:4200\n",
    "   You'll see each query as a separate task!\n",
    "\n",
    "5. Adjust batch_size based on:\n",
    "   \n",
    "   â€¢ Your database capacity (check max_connections)\n",
    "   â€¢ Query complexity (simple queries = higher batch_size)\n",
    "   â€¢ Available resources (memory, CPU)\n",
    "   \n",
    "   Start with 3, then test 5, 10, etc.\n",
    "\"\"\"\n",
    "\n",
    "print(usage_steps)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"âœ… YOU'RE READY! Copy the code above and customize for your needs.\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28690390",
   "metadata": {},
   "source": [
    "# Learning Prefect: Hello World Flow\n",
    "\n",
    "This notebook will teach you the basics of Prefect by creating a simple \"Hello World\" flow.\n",
    "\n",
    "**What you'll learn:**\n",
    "1. How to create a Prefect `@task` - a unit of work\n",
    "2. How to create a Prefect `@flow` - orchestrates tasks\n",
    "3. How to run flows and see them in the Prefect UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac94ef7b",
   "metadata": {},
   "source": [
    "## Step 1: Import Prefect and configure the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29eedd6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Prefect imported and configured to use: http://localhost:4200/api\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from prefect import flow, task\n",
    "\n",
    "# Configure to use your local Prefect server\n",
    "os.environ[\"PREFECT_API_URL\"] = \"http://localhost:4200/api\"\n",
    "\n",
    "print(\"âœ… Prefect imported and configured to use:\", os.environ[\"PREFECT_API_URL\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b08dbf",
   "metadata": {},
   "source": [
    "## Step 2: Create a simple task\n",
    "\n",
    "A **task** is a Python function decorated with `@task`. Tasks are the building blocks of flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a6ab7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Task 'print_hello' created successfully!\n",
      "Note: Tasks can only be called from within flows\n"
     ]
    }
   ],
   "source": [
    "@task\n",
    "def print_hello():\n",
    "    \"\"\"A simple task that prints Hello World\"\"\"\n",
    "    message = \"Hello World from Prefect!\"\n",
    "    print(message)\n",
    "    return message\n",
    "\n",
    "# Note: Tasks must be called from within a flow!\n",
    "# If you want to test the function directly, use: print_hello.fn()\n",
    "print(\"âœ… Task 'print_hello' created successfully!\")\n",
    "print(\"Note: Tasks can only be called from within flows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b23f44",
   "metadata": {},
   "source": [
    "## Step 3: Create a flow that uses the task\n",
    "\n",
    "A **flow** is the main orchestrator. It calls tasks and manages the workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c05be1",
   "metadata": {},
   "source": [
    "### First, let's verify connection to the Prefect server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "673e97cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Successfully connected to Prefect server!\n",
      "Server response: True\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Test connection to Prefect server\n",
    "try:\n",
    "    response = requests.get(\"http://localhost:4200/api/health\")\n",
    "    if response.status_code == 200:\n",
    "        print(\"âœ… Successfully connected to Prefect server!\")\n",
    "        print(f\"Server response: {response.json()}\")\n",
    "    else:\n",
    "        print(f\"âŒ Server returned status code: {response.status_code}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Could not connect to server: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199d5c67",
   "metadata": {},
   "source": [
    "### Now let's run a simple flow!\n",
    "\n",
    "For this to work with the external Prefect server, we'll write the flow to a Python file and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0cba5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flow Output:\n",
      "What is your favorite number?\n",
      "Favorite number: 42\n",
      "Customer IDs: ['customer30', 'customer71', 'customer27', 'customer56', 'customer72', 'customer95', 'customer47', 'customer75', 'customer71', 'customer28']\n",
      "\n",
      "\n",
      "Warnings/Errors:\n",
      "WARNING: Active profile 'local' set in the profiles file not found. The default profile will be used instead. \n",
      "19:18:10.244 | \u001b[36mINFO\u001b[0m    | prefect.engine - Created flow run\u001b[35m 'organic-spoonbill'\u001b[0m for flow\u001b[1;35m 'my-favorite-function'\u001b[0m\n",
      "19:18:10.245 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'organic-spoonbill'\u001b[0m - View at \u001b[94mhttp://localhost:4200/flow-runs/flow-run/eddf86db-c258-473a-b62d-c85f973ed21b\u001b[0m\n",
      "19:18:10.327 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'organic-spoonbill'\u001b[0m - Created task run 'get_customer_ids-0' for task 'get_customer_ids'\n",
      "19:18:10.328 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'organic-spoonbill'\u001b[0m - Executing 'get_customer_ids-0' immediately...\n",
      "19:18:10.400 | \u001b[36mINFO\u001b[0m    | Task run 'get_customer_ids-0' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "19:18:10.418 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'organic-spoonbill'\u001b[0m - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's use the existing prefect-main.py instead\n",
    "# First, let's run it!\n",
    "import subprocess\n",
    "\n",
    "result = subprocess.run(\n",
    "    [\"uv\", \"run\", \"python\", \"prefect-main.py\"],\n",
    "    cwd=\"/Users/jichong/projects/playground/BOS/prefect-poc\",\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    "    env={**os.environ, \"PREFECT_API_URL\": \"http://localhost:4200/api\"}\n",
    ")\n",
    "\n",
    "print(\"Flow Output:\")\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"\\nWarnings/Errors:\")\n",
    "    print(result.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac11a71",
   "metadata": {},
   "source": [
    "### Check the Prefect UI!\n",
    "\n",
    "ðŸŽ‰ **Success!** Your flow ran and was tracked by the Prefect server.\n",
    "\n",
    "**What just happened:**\n",
    "1. The flow `my-favorite-function` executed successfully\n",
    "2. It created and executed the task `get_customer_ids`\n",
    "3. All metadata was sent to your Prefect server at http://localhost:4200\n",
    "4. You can see the run at the URL shown above (http://localhost:4200/flow-runs/flow-run/...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce4a157",
   "metadata": {},
   "source": [
    "### Query flow runs from the Prefect API\n",
    "\n",
    "Now let's see how to programmatically query the flows and flow runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aab3d074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 recent flow runs:\n",
      "\n",
      "  â€¢ Flow: organic-spoonbill\n",
      "    State: COMPLETED\n",
      "    Start time: 2025-10-14T11:18:10.248871+00:00\n",
      "    ID: eddf86db-c258-473a-b62d-c85f973ed21b\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from prefect.client.orchestration import PrefectClient\n",
    "\n",
    "async def get_recent_flow_runs():\n",
    "    \"\"\"Query recent flow runs from the Prefect server\"\"\"\n",
    "    async with PrefectClient(api=\"http://localhost:4200/api\") as client:\n",
    "        # Get recent flow runs\n",
    "        runs = await client.read_flow_runs(limit=5)\n",
    "        \n",
    "        print(f\"Found {len(runs)} recent flow runs:\\n\")\n",
    "        for run in runs:\n",
    "            print(f\"  â€¢ Flow: {run.name}\")\n",
    "            print(f\"    State: {run.state.type if run.state else 'Unknown'}\")\n",
    "            print(f\"    Start time: {run.start_time}\")\n",
    "            print(f\"    ID: {run.id}\")\n",
    "            print()\n",
    "        \n",
    "        return runs\n",
    "\n",
    "# Run the async function\n",
    "runs = await get_recent_flow_runs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eea6c9a",
   "metadata": {},
   "source": [
    "# Part 2: Deployments and Work Pools\n",
    "\n",
    "Now let's learn about **Deployments** and **Work Pools** - how to let the Prefect server schedule and orchestrate your flows!\n",
    "\n",
    "## Concepts:\n",
    "\n",
    "- **Deployment**: Packages your flow for remote execution and scheduling\n",
    "- **Work Pool**: A queue where the server sends flow runs to be executed\n",
    "- **Worker**: An agent that pulls work from the pool and executes it\n",
    "\n",
    "Think of it like a restaurant:\n",
    "- **Deployment** = Menu item (your flow, ready to be ordered)\n",
    "- **Work Pool** = Kitchen ticket queue\n",
    "- **Worker** = Chef who takes tickets and cooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e36361",
   "metadata": {},
   "source": [
    "## Step 1: Create a Work Pool\n",
    "\n",
    "First, we need to create a work pool. A work pool is like a job queue where the server puts flow runs that need to be executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23d2e04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Work Pool Creation:\n",
      "\u001b[32mCreated work pool 'my-local-pool'.\u001b[0m\n",
      "\n",
      "WARNING: Active profile 'local' set in the profiles file not found. The default profile will be used instead. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a work pool using the Prefect CLI\n",
    "import subprocess\n",
    "\n",
    "# Create a process work pool (runs flows as local processes)\n",
    "result = subprocess.run(\n",
    "    [\"uv\", \"run\", \"prefect\", \"work-pool\", \"create\", \"my-local-pool\", \"--type\", \"process\"],\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    "    env={**os.environ, \"PREFECT_API_URL\": \"http://localhost:4200/api\"}\n",
    ")\n",
    "\n",
    "print(\"Work Pool Creation:\")\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(result.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650eec67",
   "metadata": {},
   "source": [
    "## Step 2: Create a Deployment\n",
    "\n",
    "Now let's deploy our flow. This tells the Prefect server about the flow and how to run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99aeb9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‹ Deployment Creation Guide\n",
      "======================================================================\n",
      "\n",
      "To create the deployment, run these commands in your terminal:\n",
      "\n",
      "cd /Users/jichong/projects/playground/BOS/prefect-poc\n",
      "export PREFECT_API_URL=http://localhost:4200/api\n",
      "\n",
      "# Method 1: Quick deploy (interactive)\n",
      "uv run prefect deploy deployed_flow.py:data_pipeline \\\n",
      "    --name my-data-pipeline \\\n",
      "    --pool my-local-pool\n",
      "\n",
      "# Method 2: Or use Python code\n",
      "\n",
      "\n",
      "âœ… Work pools available on server:\n",
      "   â€¢ default-agent-pool (type: prefect-agent)\n",
      "   â€¢ my-local-pool (type: process)\n",
      "\n",
      "ðŸ’¡ Next step: Start a worker (see next cell)\n"
     ]
    }
   ],
   "source": [
    "# For demonstration, let's show how to create a deployment via the command line\n",
    "# This is the recommended approach for Prefect 2.13\n",
    "\n",
    "deployment_guide = \"\"\"\n",
    "To create the deployment, run these commands in your terminal:\n",
    "\n",
    "cd /Users/jichong/projects/playground/BOS/prefect-poc\n",
    "export PREFECT_API_URL=http://localhost:4200/api\n",
    "\n",
    "# Method 1: Quick deploy (interactive)\n",
    "uv run prefect deploy deployed_flow.py:data_pipeline \\\\\n",
    "    --name my-data-pipeline \\\\\n",
    "    --pool my-local-pool\n",
    "\n",
    "# Method 2: Or use Python code\n",
    "\"\"\"\n",
    "\n",
    "print(\"ðŸ“‹ Deployment Creation Guide\")\n",
    "print(\"=\" * 70)\n",
    "print(deployment_guide)\n",
    "\n",
    "# For this demo, let's verify the work pool exists and is ready\n",
    "async def check_work_pool():\n",
    "    async with PrefectClient(api=\"http://localhost:4200/api\") as client:\n",
    "        try:\n",
    "            pools = await client.read_work_pools()\n",
    "            print(f\"\\nâœ… Work pools available on server:\")\n",
    "            for pool in pools:\n",
    "                print(f\"   â€¢ {pool.name} (type: {pool.type})\")\n",
    "            return pools\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error checking work pools: {e}\")\n",
    "            return []\n",
    "\n",
    "pools = await check_work_pool()\n",
    "\n",
    "print(\"\\nðŸ’¡ Next step: Start a worker (see next cell)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6159418",
   "metadata": {},
   "source": [
    "## Step 3: Start a Worker\n",
    "\n",
    "A worker is the \"chef\" that pulls jobs from the work pool and executes them. Without a worker, deployments won't run!\n",
    "\n",
    "**Note:** We'll start the worker in the background so it can pick up flow runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70db5c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ To start a worker, run this command in a SEPARATE TERMINAL:\n",
      "\n",
      "# Run this in a separate terminal:\n",
      "cd /Users/jichong/projects/playground/BOS/prefect-poc\n",
      "export PREFECT_API_URL=http://localhost:4200/api\n",
      "uv run prefect worker start --pool my-local-pool\n",
      "\n",
      "\n",
      "The worker will:\n",
      "  1. Connect to the Prefect server\n",
      "  2. Poll the 'my-local-pool' work pool for flow runs\n",
      "  3. Execute any flows that are scheduled\n",
      "  4. Report results back to the server\n",
      "\n",
      "âš ï¸  Leave the worker running to process flows!\n"
     ]
    }
   ],
   "source": [
    "# We'll show you the command to start a worker\n",
    "# In practice, you'd run this in a separate terminal\n",
    "\n",
    "worker_command = \"\"\"\n",
    "# Run this in a separate terminal:\n",
    "cd /Users/jichong/projects/playground/BOS/prefect-poc\n",
    "export PREFECT_API_URL=http://localhost:4200/api\n",
    "uv run prefect worker start --pool my-local-pool\n",
    "\"\"\"\n",
    "\n",
    "print(\"ðŸ”§ To start a worker, run this command in a SEPARATE TERMINAL:\")\n",
    "print(worker_command)\n",
    "print(\"\\nThe worker will:\")\n",
    "print(\"  1. Connect to the Prefect server\")\n",
    "print(\"  2. Poll the 'my-local-pool' work pool for flow runs\")\n",
    "print(\"  3. Execute any flows that are scheduled\")\n",
    "print(\"  4. Report results back to the server\")\n",
    "print(\"\\nâš ï¸  Leave the worker running to process flows!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6073e19e",
   "metadata": {},
   "source": [
    "## Step 4: Trigger a Flow Run from the Server\n",
    "\n",
    "Now that we have a deployment, we can trigger it through the server API (not by running the Python file directly!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71cc4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigger a flow run via the API\n",
    "# This creates a flow run that gets queued in the work pool\n",
    "\n",
    "result = subprocess.run([\n",
    "    \"uv\", \"run\", \"prefect\", \"deployment\", \"run\",\n",
    "    \"data-pipeline/my-data-pipeline\",  # flow-name/deployment-name\n",
    "    \"--param\", \"source=production-api\",\n",
    "    \"--param\", \"destination=warehouse\"\n",
    "],\n",
    "    cwd=\"/Users/jichong/projects/playground/BOS/prefect-poc\",\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    "    env={**os.environ, \"PREFECT_API_URL\": \"http://localhost:4200/api\"}\n",
    ")\n",
    "\n",
    "print(\"Flow Run Triggered!\")\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"\\nMessages:\")\n",
    "    print(result.stderr)\n",
    "    \n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"What happens now:\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. âœ… Server creates a flow run and puts it in the work pool queue\")\n",
    "print(\"2. â³ Worker (if running) picks up the flow run\")\n",
    "print(\"3. ðŸš€ Worker executes the flow on its machine\")\n",
    "print(\"4. ðŸ“Š Worker reports progress back to the server\")\n",
    "print(\"5. ðŸŽ‰ You can watch it in the UI at http://localhost:4200\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04399544",
   "metadata": {},
   "source": [
    "## Summary: How Deployments Work\n",
    "\n",
    "### The Full Architecture:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  YOU (Developer)                                    â”‚\n",
    "â”‚                                                     â”‚\n",
    "â”‚  1. Create flow (deployed_flow.py)                 â”‚\n",
    "â”‚  2. Deploy it: prefect deploy ...                  â”‚\n",
    "â”‚     â†’ Tells server about the flow                  â”‚\n",
    "â”‚     â†’ Server stores deployment metadata            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                    â”‚\n",
    "                    â”‚ deployment info\n",
    "                    â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  PREFECT SERVER (Docker at :4200)                  â”‚\n",
    "â”‚                                                     â”‚\n",
    "â”‚  â€¢ Stores deployment definitions                   â”‚\n",
    "â”‚  â€¢ Maintains work pool queues                      â”‚\n",
    "â”‚  â€¢ Accepts flow run requests                       â”‚\n",
    "â”‚  â€¢ Tracks all execution metadata                   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                    â”‚\n",
    "                    â”‚ flow runs queued here\n",
    "                    â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  WORK POOL: my-local-pool                          â”‚\n",
    "â”‚                                                     â”‚\n",
    "â”‚  Queue of pending flow runs waiting to execute     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                    â”‚\n",
    "                    â”‚ worker polls for work\n",
    "                    â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  WORKER (prefect worker start --pool ...)          â”‚\n",
    "â”‚                                                     â”‚\n",
    "â”‚  â€¢ Polls work pool every few seconds               â”‚\n",
    "â”‚  â€¢ Picks up flow runs from queue                   â”‚\n",
    "â”‚  â€¢ Executes the flow on its machine                â”‚\n",
    "â”‚  â€¢ Sends progress/logs back to server              â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Key Differences from Before:\n",
    "\n",
    "| Aspect | Direct Execution | With Deployment |\n",
    "|--------|-----------------|-----------------|\n",
    "| **How to run** | `python flow.py` | Server API or schedule |\n",
    "| **Who triggers** | You manually | Server (on schedule or API call) |\n",
    "| **Where code runs** | Your terminal | Worker machine |\n",
    "| **Code location** | Must be local | Must be accessible to worker |\n",
    "| **Scheduling** | Manual only | Can schedule (cron, interval, etc.) |\n",
    "| **Production ready** | No | Yes |\n",
    "\n",
    "### Use Cases:\n",
    "\n",
    "- **Direct execution** (what we did first): Development, testing, one-off runs\n",
    "- **Deployments + Workers**: Production, scheduled jobs, distributed execution, team workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823822b5",
   "metadata": {},
   "source": [
    "## ðŸŽ“ Try it Yourself!\n",
    "\n",
    "Here's the complete workflow to see deployments in action:\n",
    "\n",
    "### 1ï¸âƒ£ Create the deployment (in a terminal):\n",
    "```bash\n",
    "cd /Users/jichong/projects/playground/BOS/prefect-poc\n",
    "export PREFECT_API_URL=http://localhost:4200/api\n",
    "uv run prefect deploy deployed_flow.py:data_pipeline --name my-data-pipeline --pool my-local-pool\n",
    "```\n",
    "\n",
    "### 2ï¸âƒ£ Start a worker (in another terminal):\n",
    "```bash\n",
    "cd /Users/jichong/projects/playground/BOS/prefect-poc\n",
    "export PREFECT_API_URL=http://localhost:4200/api\n",
    "uv run prefect worker start --pool my-local-pool\n",
    "```\n",
    "\n",
    "### 3ï¸âƒ£ Trigger a flow run (in a third terminal or from the UI):\n",
    "```bash\n",
    "# Via CLI:\n",
    "uv run prefect deployment run data-pipeline/my-data-pipeline\n",
    "\n",
    "# Or open http://localhost:4200/deployments and click \"Run\"\n",
    "```\n",
    "\n",
    "### 4ï¸âƒ£ Watch it execute!\n",
    "- The worker terminal will show the flow executing\n",
    "- The UI at http://localhost:4200 will show real-time progress\n",
    "- All logs and results are tracked by the server\n",
    "\n",
    "---\n",
    "\n",
    "### What you've learned:\n",
    "\n",
    "âœ… **Direct execution** (`python flow.py`):\n",
    "- Code runs on your machine\n",
    "- You trigger it manually\n",
    "- Good for development/testing\n",
    "\n",
    "âœ… **Deployments + Workers**:\n",
    "- Server orchestrates the execution\n",
    "- Workers pull jobs from work pools\n",
    "- Can schedule flows, handle retries, distribute work\n",
    "- Production-ready approach\n",
    "\n",
    "This is how Prefect powers production data pipelines at scale! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e30f6b",
   "metadata": {},
   "source": [
    "# Part 3: Blocks and Remote Storage\n",
    "\n",
    "## What are Blocks?\n",
    "\n",
    "**Blocks** are Prefect's way of storing reusable configuration and credentials. Think of them as secure configuration objects that can be:\n",
    "- Created once and reused across many flows\n",
    "- Stored on the Prefect server\n",
    "- Referenced by name in deployments\n",
    "- Used to connect to external systems (databases, cloud storage, git repos, etc.)\n",
    "\n",
    "### Common Block Types:\n",
    "- **Git Repository Blocks**: Connect to GitHub, GitLab, Bitbucket\n",
    "- **Storage Blocks**: S3, GCS, Azure Blob Storage\n",
    "- **Secret Blocks**: Store passwords, API keys\n",
    "- **Infrastructure Blocks**: Docker, Kubernetes, cloud compute\n",
    "\n",
    "### Why Blocks Matter for Private Bitbucket:\n",
    "When your code is in a private Git repository:\n",
    "1. **Worker needs access** to pull the code before running it\n",
    "2. **Credentials must be secure** (not hardcoded in code)\n",
    "3. **Blocks solve both problems** by storing Git credentials on the server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9080f243",
   "metadata": {},
   "source": [
    "## Step 1: Create a Bitbucket Repository Block\n",
    "\n",
    "Let's create a block that stores credentials for your private Bitbucket server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af9ce13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Bitbucket repository block programmatically\n",
    "\n",
    "from prefect.blocks.system import Secret\n",
    "from prefect_bitbucket.credentials import BitBucketCredentials\n",
    "from prefect_bitbucket.repository import BitBucketRepository\n",
    "\n",
    "async def create_bitbucket_block():\n",
    "    \"\"\"\n",
    "    Create a Bitbucket repository block for private repo access.\n",
    "    This block will be stored on the Prefect server.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Create a secret block for the password/token\n",
    "    # (In production, you'd set this more securely)\n",
    "    try:\n",
    "        bitbucket_token = Secret(value=\"your-app-password-or-token\")\n",
    "        await bitbucket_token.save(name=\"bitbucket-token\", overwrite=True)\n",
    "        print(\"âœ… Created secret block: bitbucket-token\")\n",
    "    except Exception as e:\n",
    "        print(f\"Note: {e}\")\n",
    "    \n",
    "    # Step 2: Create credentials block\n",
    "    try:\n",
    "        credentials = BitBucketCredentials(\n",
    "            username=\"your-username\",\n",
    "            token=bitbucket_token  # Reference to the secret block\n",
    "        )\n",
    "        await credentials.save(name=\"my-bitbucket-creds\", overwrite=True)\n",
    "        print(\"âœ… Created credentials block: my-bitbucket-creds\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  BitBucket block type not installed: {e}\")\n",
    "        print(\"   You would need to: pip install prefect-bitbucket\")\n",
    "    \n",
    "    # Step 3: Create repository block\n",
    "    try:\n",
    "        repo_block = BitBucketRepository(\n",
    "            repository=\"https://bitbucket.mycompany.com/scm/project/repo.git\",\n",
    "            credentials=credentials,\n",
    "            reference=\"main\"  # branch name\n",
    "        )\n",
    "        await repo_block.save(name=\"my-private-repo\", overwrite=True)\n",
    "        print(\"âœ… Created repository block: my-private-repo\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Could not create repo block: {e}\")\n",
    "    \n",
    "    print(\"\\nðŸ“ Block created and stored on Prefect server!\")\n",
    "    print(\"   Workers can now use this block to pull code from private Bitbucket\")\n",
    "\n",
    "# Note: This will fail without prefect-bitbucket installed\n",
    "# Showing the pattern for demonstration\n",
    "print(\"Example: Creating Bitbucket Block\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nâš ï¸  Note: This requires 'prefect-bitbucket' to be installed:\")\n",
    "print(\"   pip install prefect-bitbucket\")\n",
    "print(\"\\nFor demonstration purposes, let's see what blocks we currently have:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79dfd69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Available Block Types on Server:\n",
      "   Found 37 block types\n",
      "\n",
      "   â€¢ AWS Credentials\n",
      "     Block used to manage authentication with AWS. AWS authentication is\n",
      "handled via ...\n",
      "   â€¢ Azure\n",
      "     Store data as a file on Azure Datalake and Azure Blob Storage....\n",
      "   â€¢ Azure Blob Storage Credentials\n",
      "     Block used to manage Blob Storage authentication with Azure.\n",
      "Azure authenticatio...\n",
      "   â€¢ Azure Container Instance Credentials\n",
      "     Block used to manage Azure Container Instances authentication. Stores Azure Serv...\n",
      "   â€¢ Azure Container Instance Job\n",
      "     Run tasks using Azure Container Instances. Note this block is experimental. The ...\n",
      "   â€¢ Azure Cosmos DB Credentials\n",
      "     Block used to manage Cosmos DB authentication with Azure.\n",
      "Azure authentication i...\n",
      "   â€¢ AzureML Credentials\n",
      "     Block used to manage authentication with AzureML. Azure authentication is\n",
      "handle...\n",
      "   â€¢ BigQuery Warehouse\n",
      "     A block for querying a database with BigQuery.\n",
      "\n",
      "Upon instantiating, a connection...\n",
      "   â€¢ Custom Webhook\n",
      "     Enables sending notifications via any custom webhook.\n",
      "\n",
      "All nested string param c...\n",
      "   â€¢ Databricks Credentials\n",
      "     Block used to manage Databricks authentication....\n",
      "\n",
      "ðŸ’¡ These blocks can be used in deployments to store credentials and config\n"
     ]
    }
   ],
   "source": [
    "# List all available blocks on the server\n",
    "async def list_blocks():\n",
    "    \"\"\"Query all blocks stored on the Prefect server\"\"\"\n",
    "    async with PrefectClient(api=\"http://localhost:4200/api\") as client:\n",
    "        # Get block types\n",
    "        block_types = await client.read_block_types()\n",
    "        \n",
    "        print(f\"ðŸ“¦ Available Block Types on Server:\")\n",
    "        print(f\"   Found {len(block_types)} block types\\n\")\n",
    "        \n",
    "        for bt in block_types[:10]:  # Show first 10\n",
    "            print(f\"   â€¢ {bt.name}\")\n",
    "            if bt.description:\n",
    "                print(f\"     {bt.description[:80]}...\")\n",
    "        \n",
    "        return block_types\n",
    "\n",
    "blocks = await list_blocks()\n",
    "print(f\"\\nðŸ’¡ These blocks can be used in deployments to store credentials and config\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff52e4f1",
   "metadata": {},
   "source": [
    "## Step 2: How Deployments Work with Private Bitbucket\n",
    "\n",
    "Here's the complete architecture when using a private Bitbucket server:\n",
    "\n",
    "### The Flow:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  BITBUCKET SERVER (Private)                             â”‚\n",
    "â”‚  https://bitbucket.mycompany.com                        â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  ðŸ“ Your Repository:                                    â”‚\n",
    "â”‚    â€¢ flow.py (your flow code)                          â”‚\n",
    "â”‚    â€¢ requirements.txt                                   â”‚\n",
    "â”‚    â€¢ prefect.yaml (deployment config)                  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                    â–²\n",
    "                    â”‚ 1. Git clone with credentials\n",
    "                    â”‚\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  PREFECT SERVER (Your deployment)                       â”‚\n",
    "â”‚  Could be: Docker, cloud, or on-premises                â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  Stores:                                                â”‚\n",
    "â”‚    â€¢ ðŸ” Bitbucket credentials (Block)                  â”‚\n",
    "â”‚    â€¢ ðŸ“‹ Deployment definitions                         â”‚\n",
    "â”‚    â€¢ ðŸŽ¯ Work pool configurations                       â”‚\n",
    "â”‚    â€¢ ðŸ“Š Flow run metadata                              â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                    â”‚\n",
    "                    â”‚ 2. Worker polls for work\n",
    "                    â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  WORKER (Running on your infrastructure)                â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  When a flow run is scheduled:                          â”‚\n",
    "â”‚    1. Worker gets job from work pool                   â”‚\n",
    "â”‚    2. Worker loads Bitbucket credentials from block     â”‚\n",
    "â”‚    3. Worker clones code from private repo              â”‚\n",
    "â”‚    4. Worker creates Python environment                 â”‚\n",
    "â”‚    5. Worker installs dependencies                      â”‚\n",
    "â”‚    6. Worker executes the flow                          â”‚\n",
    "â”‚    7. Worker sends results back to server               â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Key Points:\n",
    "\n",
    "1. **Prefect server doesn't execute code** - It only stores metadata\n",
    "2. **Worker pulls code** - Worker machine needs network access to Bitbucket\n",
    "3. **Credentials stored securely** - Blocks keep secrets on the server\n",
    "4. **Code is ephemeral** - Worker clones fresh code for each run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3e8226",
   "metadata": {},
   "source": [
    "## Step 3: Creating a Deployment with Bitbucket Storage\n",
    "\n",
    "Here's how to deploy a flow that lives in a private Bitbucket repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9d0ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example deployment configuration with Bitbucket storage\n",
    "\n",
    "deployment_example = \"\"\"\n",
    "# prefect.yaml - Deployment configuration file in your Bitbucket repo\n",
    "\n",
    "name: production-deployment\n",
    "prefect-version: 2.13.7\n",
    "\n",
    "# Define where the code comes from\n",
    "pull:\n",
    "  - prefect.deployments.steps.git_clone:\n",
    "      repository: https://bitbucket.mycompany.com/scm/project/myflows.git\n",
    "      branch: main\n",
    "      credentials: \"{{ prefect.blocks.bitbucket-credentials.my-bitbucket-creds }}\"\n",
    "\n",
    "# Define deployments\n",
    "deployments:\n",
    "  - name: prod-data-pipeline\n",
    "    entrypoint: flows/data_pipeline.py:data_pipeline\n",
    "    work_pool:\n",
    "      name: prod-worker-pool\n",
    "    schedule:\n",
    "      cron: \"0 2 * * *\"  # Run at 2 AM daily\n",
    "    parameters:\n",
    "      env: production\n",
    "\"\"\"\n",
    "\n",
    "print(\"ðŸ“„ Example prefect.yaml with Bitbucket Storage\")\n",
    "print(\"=\" * 70)\n",
    "print(deployment_example)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"How to deploy from your Bitbucket repository:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "deployment_steps = \"\"\"\n",
    "1ï¸âƒ£ Push your code to Bitbucket:\n",
    "   â€¢ flows/data_pipeline.py (your flow code)\n",
    "   â€¢ prefect.yaml (deployment config - shown above)\n",
    "   â€¢ requirements.txt (Python dependencies)\n",
    "\n",
    "2ï¸âƒ£ Create the Bitbucket credentials block (on Prefect server):\n",
    "   uv run prefect block register -m prefect_bitbucket\n",
    "   # Then create the block via UI or Python as shown earlier\n",
    "\n",
    "3ï¸âƒ£ Deploy from your local machine (one time):\n",
    "   cd /path/to/your/repo\n",
    "   export PREFECT_API_URL=http://your-prefect-server:4200/api\n",
    "   uv run prefect deploy --all\n",
    "\n",
    "   This tells the server about your deployment and where to find the code.\n",
    "\n",
    "4ï¸âƒ£ Start workers (on your infrastructure):\n",
    "   # These can run anywhere with access to Bitbucket\n",
    "   export PREFECT_API_URL=http://your-prefect-server:4200/api\n",
    "   uv run prefect worker start --pool prod-worker-pool\n",
    "\n",
    "5ï¸âƒ£ When a flow run is triggered:\n",
    "   â€¢ Worker gets the job from the server\n",
    "   â€¢ Worker uses the Bitbucket block to clone the repo\n",
    "   â€¢ Worker runs the flow from the cloned code\n",
    "   â€¢ All logs go back to the server\n",
    "\"\"\"\n",
    "\n",
    "print(deployment_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd9cf0a",
   "metadata": {},
   "source": [
    "## Step 4: Answering Your Questions\n",
    "\n",
    "### Q: \"If I deploy my own Prefect server, how will it connect to the private Bitbucket server?\"\n",
    "\n",
    "**Answer: The Prefect server DOESN'T connect to Bitbucket. The WORKER does!**\n",
    "\n",
    "Here's the breakdown:\n",
    "\n",
    "#### What the Prefect Server Does:\n",
    "- âœ… Stores the Bitbucket credentials (in a Block)\n",
    "- âœ… Stores deployment metadata (which repo, which branch)\n",
    "- âœ… Manages the work queue\n",
    "- âŒ **Does NOT** clone code\n",
    "- âŒ **Does NOT** execute flows\n",
    "- âŒ **Does NOT** need network access to Bitbucket\n",
    "\n",
    "#### What the Worker Does:\n",
    "- âœ… Runs on your infrastructure (can be same server, different server, laptop, cloud VM)\n",
    "- âœ… **NEEDS** network access to your private Bitbucket server\n",
    "- âœ… Pulls credentials from the Prefect server (via API)\n",
    "- âœ… Uses credentials to clone code from Bitbucket\n",
    "- âœ… Executes the flow\n",
    "- âœ… Sends results back to Prefect server\n",
    "\n",
    "### Network Requirements:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Your Network / VPN                              â”‚\n",
    "â”‚                                                  â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
    "â”‚  â”‚ Prefect Server â”‚     â”‚ Bitbucket Server â”‚   â”‚\n",
    "â”‚  â”‚ :4200          â”‚     â”‚ (Private)        â”‚   â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
    "â”‚         â–²                        â–²              â”‚\n",
    "â”‚         â”‚                        â”‚              â”‚\n",
    "â”‚         â”‚  API calls             â”‚ Git clone    â”‚\n",
    "â”‚         â”‚  (get credentials,     â”‚ (needs auth) â”‚\n",
    "â”‚         â”‚   report status)       â”‚              â”‚\n",
    "â”‚         â”‚                        â”‚              â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”      â”‚\n",
    "â”‚  â”‚           WORKER                      â”‚      â”‚\n",
    "â”‚  â”‚  â€¢ Needs access to BOTH servers       â”‚      â”‚\n",
    "â”‚  â”‚  â€¢ Can be same or different machine   â”‚      â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Security Best Practices:\n",
    "\n",
    "1. **Worker Placement**: Deploy workers in the same network zone as Bitbucket\n",
    "2. **Credentials**: Use SSH keys or app passwords, not your personal password\n",
    "3. **Least Privilege**: Give the Bitbucket credentials read-only access\n",
    "4. **Network Policies**: Workers need HTTPS/SSH to Bitbucket, HTTPS to Prefect server\n",
    "5. **Secrets**: Never commit credentials to git - use Blocks!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f238a7",
   "metadata": {},
   "source": [
    "## Real-World Example: Production Setup\n",
    "\n",
    "Let's walk through a complete production setup:\n",
    "\n",
    "### Scenario:\n",
    "- Company has private Bitbucket server at `bitbucket.company.com`\n",
    "- Data engineering team writes flows in `data-team/etl-flows` repo\n",
    "- Prefect server deployed on `prefect.company.com`\n",
    "- Workers run on Kubernetes in the same VPC\n",
    "\n",
    "### Setup Steps:\n",
    "\n",
    "#### 1. Install Prefect Bitbucket Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33c6dc41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PRODUCTION SETUP: Prefect + Private Bitbucket\n",
      "================================================================================\n",
      "\n",
      "STEP 1: Install Prefect Bitbucket Integration\n",
      "----------------------------------------------\n",
      "On your development machine and in worker Docker images:\n",
      "\n",
      "    pip install prefect-bitbucket\n",
      "\n",
      "This provides:\n",
      "  â€¢ BitBucketCredentials block\n",
      "  â€¢ BitBucketRepository block\n",
      "  â€¢ Git clone steps for deployments\n",
      "\n",
      "\n",
      "STEP 2: Create Bitbucket App Password\n",
      "--------------------------------------\n",
      "On bitbucket.company.com:\n",
      "  1. Go to Personal Settings â†’ App Passwords\n",
      "  2. Create new app password with 'Repository Read' permission\n",
      "  3. Save the password securely (you'll need it once)\n",
      "\n",
      "\n",
      "STEP 3: Register Block Types & Create Blocks\n",
      "---------------------------------------------\n",
      "# Register the block types with your Prefect server\n",
      "export PREFECT_API_URL=https://prefect.company.com/api\n",
      "prefect block register -m prefect_bitbucket\n",
      "\n",
      "# Create credentials block (via Python or UI)\n",
      "from prefect_bitbucket.credentials import BitBucketCredentials\n",
      "from prefect.blocks.system import Secret\n",
      "\n",
      "async def setup_blocks():\n",
      "    # Create secret for the app password\n",
      "    token = Secret(value=\"your-app-password-here\")\n",
      "    await token.save(\"bitbucket-token\")\n",
      "    \n",
      "    # Create credentials\n",
      "    creds = BitBucketCredentials(\n",
      "        username=\"serviceaccount@company.com\",\n",
      "        token=token\n",
      "    )\n",
      "    await creds.save(\"company-bitbucket-creds\")\n",
      "\n",
      "# Or use the Prefect UI:\n",
      "# Navigate to Blocks â†’ + â†’ BitBucketCredentials\n",
      "\n",
      "\n",
      "STEP 4: Structure Your Bitbucket Repository\n",
      "--------------------------------------------\n",
      "data-team/etl-flows/\n",
      "â”œâ”€â”€ flows/\n",
      "â”‚   â”œâ”€â”€ daily_etl.py\n",
      "â”‚   â”œâ”€â”€ weekly_report.py\n",
      "â”‚   â””â”€â”€ __init__.py\n",
      "â”œâ”€â”€ requirements.txt\n",
      "â””â”€â”€ prefect.yaml\n",
      "\n",
      "# requirements.txt\n",
      "prefect==2.13.7\n",
      "pandas\n",
      "sqlalchemy\n",
      "psycopg2-binary\n",
      "\n",
      "# prefect.yaml\n",
      "name: etl-flows\n",
      "prefect-version: 2.13.7\n",
      "\n",
      "pull:\n",
      "  - prefect.deployments.steps.git_clone:\n",
      "      repository: https://bitbucket.company.com/scm/data-team/etl-flows.git\n",
      "      branch: \"{{ branch }}\"\n",
      "      credentials: \"{{ prefect.blocks.bitbucket-credentials.company-bitbucket-creds }}\"\n",
      "\n",
      "deployments:\n",
      "  - name: daily-etl-prod\n",
      "    entrypoint: flows/daily_etl.py:main_flow\n",
      "    work_pool:\n",
      "      name: k8s-prod-pool\n",
      "    schedule:\n",
      "      cron: \"0 1 * * *\"\n",
      "    parameters:\n",
      "      environment: production\n",
      "\n",
      "\n",
      "STEP 5: Deploy from Your Machine (One Time)\n",
      "--------------------------------------------\n",
      "cd /path/to/etl-flows\n",
      "export PREFECT_API_URL=https://prefect.company.com/api\n",
      "prefect deploy --all\n",
      "\n",
      "This registers the deployment with the server. The server now knows:\n",
      "  â€¢ Where to find the code (Bitbucket URL)\n",
      "  â€¢ Which credentials to use (the block name)\n",
      "  â€¢ Which branch to use (main)\n",
      "  â€¢ Schedule and parameters\n",
      "\n",
      "\n",
      "STEP 6: Run Workers (Continuously)\n",
      "-----------------------------------\n",
      "Workers run on your infrastructure (K8s, VMs, etc)\n",
      "\n",
      "# Kubernetes deployment.yaml\n",
      "apiVersion: apps/v1\n",
      "kind: Deployment\n",
      "metadata:\n",
      "  name: prefect-worker\n",
      "spec:\n",
      "  replicas: 3\n",
      "  template:\n",
      "    spec:\n",
      "      containers:\n",
      "      - name: worker\n",
      "        image: prefecthq/prefect:2.13.7-python3.10\n",
      "        command: [\"prefect\", \"worker\", \"start\"]\n",
      "        args: [\"--pool\", \"k8s-prod-pool\"]\n",
      "        env:\n",
      "        - name: PREFECT_API_URL\n",
      "          value: \"https://prefect.company.com/api\"\n",
      "\n",
      "Workers will:\n",
      "  1. Poll the work pool for jobs\n",
      "  2. When a job arrives, load credentials from the server\n",
      "  3. Clone code from Bitbucket using those credentials\n",
      "  4. Execute the flow\n",
      "  5. Report results back\n",
      "\n",
      "\n",
      "STEP 7: Trigger & Monitor\n",
      "--------------------------\n",
      "Flows run automatically on schedule, or trigger manually:\n",
      "\n",
      "  prefect deployment run daily-etl-prod/daily-etl-prod\n",
      "\n",
      "Monitor at: https://prefect.company.com/deployments\n",
      "\n",
      "\n",
      "SECURITY NOTES:\n",
      "--------------\n",
      "âœ… Credentials never in code - stored in Blocks on server\n",
      "âœ… Workers pull fresh code each run - no stale code\n",
      "âœ… Use service account with read-only Bitbucket access\n",
      "âœ… Workers need network access to both Prefect server AND Bitbucket\n",
      "âœ… Prefect server only needs to be accessible to workers (not Bitbucket)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Complete production example walkthrough\n",
    "\n",
    "production_guide = \"\"\"\n",
    "================================================================================\n",
    "PRODUCTION SETUP: Prefect + Private Bitbucket\n",
    "================================================================================\n",
    "\n",
    "STEP 1: Install Prefect Bitbucket Integration\n",
    "----------------------------------------------\n",
    "On your development machine and in worker Docker images:\n",
    "\n",
    "    pip install prefect-bitbucket\n",
    "\n",
    "This provides:\n",
    "  â€¢ BitBucketCredentials block\n",
    "  â€¢ BitBucketRepository block\n",
    "  â€¢ Git clone steps for deployments\n",
    "\n",
    "\n",
    "STEP 2: Create Bitbucket App Password\n",
    "--------------------------------------\n",
    "On bitbucket.company.com:\n",
    "  1. Go to Personal Settings â†’ App Passwords\n",
    "  2. Create new app password with 'Repository Read' permission\n",
    "  3. Save the password securely (you'll need it once)\n",
    "\n",
    "\n",
    "STEP 3: Register Block Types & Create Blocks\n",
    "---------------------------------------------\n",
    "# Register the block types with your Prefect server\n",
    "export PREFECT_API_URL=https://prefect.company.com/api\n",
    "prefect block register -m prefect_bitbucket\n",
    "\n",
    "# Create credentials block (via Python or UI)\n",
    "from prefect_bitbucket.credentials import BitBucketCredentials\n",
    "from prefect.blocks.system import Secret\n",
    "\n",
    "async def setup_blocks():\n",
    "    # Create secret for the app password\n",
    "    token = Secret(value=\"your-app-password-here\")\n",
    "    await token.save(\"bitbucket-token\")\n",
    "    \n",
    "    # Create credentials\n",
    "    creds = BitBucketCredentials(\n",
    "        username=\"serviceaccount@company.com\",\n",
    "        token=token\n",
    "    )\n",
    "    await creds.save(\"company-bitbucket-creds\")\n",
    "\n",
    "# Or use the Prefect UI:\n",
    "# Navigate to Blocks â†’ + â†’ BitBucketCredentials\n",
    "\n",
    "\n",
    "STEP 4: Structure Your Bitbucket Repository\n",
    "--------------------------------------------\n",
    "data-team/etl-flows/\n",
    "â”œâ”€â”€ flows/\n",
    "â”‚   â”œâ”€â”€ daily_etl.py\n",
    "â”‚   â”œâ”€â”€ weekly_report.py\n",
    "â”‚   â””â”€â”€ __init__.py\n",
    "â”œâ”€â”€ requirements.txt\n",
    "â””â”€â”€ prefect.yaml\n",
    "\n",
    "# requirements.txt\n",
    "prefect==2.13.7\n",
    "pandas\n",
    "sqlalchemy\n",
    "psycopg2-binary\n",
    "\n",
    "# prefect.yaml\n",
    "name: etl-flows\n",
    "prefect-version: 2.13.7\n",
    "\n",
    "pull:\n",
    "  - prefect.deployments.steps.git_clone:\n",
    "      repository: https://bitbucket.company.com/scm/data-team/etl-flows.git\n",
    "      branch: \"{{ branch }}\"\n",
    "      credentials: \"{{ prefect.blocks.bitbucket-credentials.company-bitbucket-creds }}\"\n",
    "\n",
    "deployments:\n",
    "  - name: daily-etl-prod\n",
    "    entrypoint: flows/daily_etl.py:main_flow\n",
    "    work_pool:\n",
    "      name: k8s-prod-pool\n",
    "    schedule:\n",
    "      cron: \"0 1 * * *\"\n",
    "    parameters:\n",
    "      environment: production\n",
    "\n",
    "\n",
    "STEP 5: Deploy from Your Machine (One Time)\n",
    "--------------------------------------------\n",
    "cd /path/to/etl-flows\n",
    "export PREFECT_API_URL=https://prefect.company.com/api\n",
    "prefect deploy --all\n",
    "\n",
    "This registers the deployment with the server. The server now knows:\n",
    "  â€¢ Where to find the code (Bitbucket URL)\n",
    "  â€¢ Which credentials to use (the block name)\n",
    "  â€¢ Which branch to use (main)\n",
    "  â€¢ Schedule and parameters\n",
    "\n",
    "\n",
    "STEP 6: Run Workers (Continuously)\n",
    "-----------------------------------\n",
    "Workers run on your infrastructure (K8s, VMs, etc)\n",
    "\n",
    "# Kubernetes deployment.yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: prefect-worker\n",
    "spec:\n",
    "  replicas: 3\n",
    "  template:\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: worker\n",
    "        image: prefecthq/prefect:2.13.7-python3.10\n",
    "        command: [\"prefect\", \"worker\", \"start\"]\n",
    "        args: [\"--pool\", \"k8s-prod-pool\"]\n",
    "        env:\n",
    "        - name: PREFECT_API_URL\n",
    "          value: \"https://prefect.company.com/api\"\n",
    "\n",
    "Workers will:\n",
    "  1. Poll the work pool for jobs\n",
    "  2. When a job arrives, load credentials from the server\n",
    "  3. Clone code from Bitbucket using those credentials\n",
    "  4. Execute the flow\n",
    "  5. Report results back\n",
    "\n",
    "\n",
    "STEP 7: Trigger & Monitor\n",
    "--------------------------\n",
    "Flows run automatically on schedule, or trigger manually:\n",
    "\n",
    "  prefect deployment run daily-etl-prod/daily-etl-prod\n",
    "\n",
    "Monitor at: https://prefect.company.com/deployments\n",
    "\n",
    "\n",
    "SECURITY NOTES:\n",
    "--------------\n",
    "âœ… Credentials never in code - stored in Blocks on server\n",
    "âœ… Workers pull fresh code each run - no stale code\n",
    "âœ… Use service account with read-only Bitbucket access\n",
    "âœ… Workers need network access to both Prefect server AND Bitbucket\n",
    "âœ… Prefect server only needs to be accessible to workers (not Bitbucket)\n",
    "\"\"\"\n",
    "\n",
    "print(production_guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2616de",
   "metadata": {},
   "source": [
    "## Summary: Key Takeaways\n",
    "\n",
    "### ðŸ”‘ Blocks Explained:\n",
    "- **What**: Reusable configuration objects stored on Prefect server\n",
    "- **Why**: Securely store credentials, avoid hardcoding secrets\n",
    "- **How**: Create once, reference by name in deployments\n",
    "- **Types**: Git repos, databases, cloud storage, secrets, infrastructure\n",
    "\n",
    "### ðŸ”’ Private Bitbucket + Prefect Architecture:\n",
    "\n",
    "| Component | Role | Needs Access To |\n",
    "|-----------|------|-----------------|\n",
    "| **Bitbucket Server** | Stores your flow code | Nothing (it's the source) |\n",
    "| **Prefect Server** | Stores metadata, credentials, schedules | Nothing (workers connect to it) |\n",
    "| **Worker** | Executes flows | **Both** Bitbucket AND Prefect Server |\n",
    "\n",
    "### ðŸ“Š The Complete Flow:\n",
    "\n",
    "1. **Developer** pushes code to private Bitbucket\n",
    "2. **Developer** creates Bitbucket credentials Block on Prefect server\n",
    "3. **Developer** runs `prefect deploy` to register deployment\n",
    "4. **Prefect Server** stores deployment metadata (not the code!)\n",
    "5. **Worker** polls Prefect server for work\n",
    "6. **Worker** gets credentials from Prefect server (via Block)\n",
    "7. **Worker** clones code from Bitbucket using those credentials\n",
    "8. **Worker** executes the flow\n",
    "9. **Worker** sends results back to Prefect server\n",
    "10. **You** view results in Prefect UI\n",
    "\n",
    "### âœ… What You've Learned:\n",
    "\n",
    "1. **Blocks** are Prefect's way to store reusable config and credentials\n",
    "2. **Prefect server** never executes code or clones repos\n",
    "3. **Workers** do all the heavy lifting (clone + execute)\n",
    "4. **Network access**: Workers need access to both Prefect server and Bitbucket\n",
    "5. **Security**: Credentials in Blocks, never in code\n",
    "6. **Deployment**: Code stays in git, workers pull it fresh each time\n",
    "\n",
    "### ðŸš€ Next Steps:\n",
    "\n",
    "1. Install `prefect-bitbucket`: `pip install prefect-bitbucket`\n",
    "2. Create a service account in Bitbucket with read-only access\n",
    "3. Create Bitbucket credentials Block on your Prefect server\n",
    "4. Add `prefect.yaml` to your repository\n",
    "5. Deploy: `prefect deploy --all`\n",
    "6. Start workers in your infrastructure\n",
    "7. Watch flows run from your private Bitbucket repo!\n",
    "\n",
    "---\n",
    "\n",
    "**Remember**: The beauty of this architecture is that your Prefect server can be anywhere (cloud, on-prem, DMZ), and your workers just need network access to both the server and Bitbucket. The server itself never needs to reach Bitbucket! ðŸŽ¯"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prefect-poc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
