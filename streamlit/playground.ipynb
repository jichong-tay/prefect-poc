{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4edd1b9",
   "metadata": {},
   "source": [
    "# Part 4: Parallel Task Execution - Running Multiple SQL Queries Concurrently\n",
    "\n",
    "## The Problem:\n",
    "You have a loop that runs SQL queries one at a time (sequential):\n",
    "```python\n",
    "for sql_query in queries:\n",
    "    result = execute_query(sql_query)  # Slow! Waits for each query\n",
    "```\n",
    "\n",
    "## The Solution:\n",
    "Run multiple queries in parallel (3 at a time) to speed up execution.\n",
    "\n",
    "**What you'll learn:**\n",
    "1. How to use Prefect's `.submit()` for async task execution\n",
    "2. How to limit concurrency (max 3 tasks at once)\n",
    "3. How to collect results from parallel tasks\n",
    "4. Best practices for database connection pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea2c6fd",
   "metadata": {},
   "source": [
    "## Approach 1: Using `.submit()` with Manual Batching\n",
    "\n",
    "This approach runs exactly 3 queries at a time by batching them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0aa570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prefect import flow, task\n",
    "from typing import List, Dict, Any\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Simulate SQL query execution\n",
    "@task(name=\"execute_sql_query\")\n",
    "def execute_sql_query(query_name: str, query: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Simulates executing a SQL query.\n",
    "    In production, this would connect to your database.\n",
    "    \"\"\"\n",
    "    print(f\"🔵 Starting query: {query_name} at {datetime.now().strftime('%H:%M:%S')}\")\n",
    "    \n",
    "    # Simulate query execution time (1-3 seconds)\n",
    "    import random\n",
    "    execution_time = random.uniform(1, 3)\n",
    "    time.sleep(execution_time)\n",
    "    \n",
    "    # Simulate results\n",
    "    result = {\n",
    "        \"query_name\": query_name,\n",
    "        \"query\": query,\n",
    "        \"row_count\": random.randint(100, 1000),\n",
    "        \"execution_time\": execution_time,\n",
    "        \"completed_at\": datetime.now().strftime('%H:%M:%S')\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ Completed query: {query_name} ({execution_time:.2f}s)\")\n",
    "    return result\n",
    "\n",
    "\n",
    "@flow(name=\"parallel_sql_batch_approach\")\n",
    "def run_queries_in_batches(queries: List[Dict[str, str]], batch_size: int = 3):\n",
    "    \"\"\"\n",
    "    Run SQL queries in batches of N at a time.\n",
    "    \n",
    "    Args:\n",
    "        queries: List of dicts with 'name' and 'sql' keys\n",
    "        batch_size: Number of queries to run concurrently (default: 3)\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    \n",
    "    # Process queries in batches\n",
    "    for i in range(0, len(queries), batch_size):\n",
    "        batch = queries[i:i + batch_size]\n",
    "        print(f\"\\n📦 Processing batch {i//batch_size + 1} ({len(batch)} queries)\")\n",
    "        \n",
    "        # Submit all tasks in this batch\n",
    "        futures = []\n",
    "        for q in batch:\n",
    "            future = execute_sql_query.submit(q[\"name\"], q[\"sql\"])\n",
    "            futures.append(future)\n",
    "        \n",
    "        # Wait for all tasks in this batch to complete\n",
    "        batch_results = [future.result() for future in futures]\n",
    "        all_results.extend(batch_results)\n",
    "        \n",
    "        print(f\"✅ Batch {i//batch_size + 1} complete\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "\n",
    "# Example: Simulate reading queries from a config file\n",
    "sample_queries = [\n",
    "    {\"name\": \"customer_orders\", \"sql\": \"SELECT * FROM orders WHERE customer_id = 123\"},\n",
    "    {\"name\": \"product_sales\", \"sql\": \"SELECT product_id, SUM(sales) FROM sales GROUP BY product_id\"},\n",
    "    {\"name\": \"inventory_check\", \"sql\": \"SELECT * FROM inventory WHERE stock < 10\"},\n",
    "    {\"name\": \"user_activity\", \"sql\": \"SELECT user_id, COUNT(*) FROM activity GROUP BY user_id\"},\n",
    "    {\"name\": \"revenue_report\", \"sql\": \"SELECT date, SUM(revenue) FROM transactions GROUP BY date\"},\n",
    "    {\"name\": \"top_customers\", \"sql\": \"SELECT customer_id, total_spent FROM customers ORDER BY total_spent DESC LIMIT 100\"},\n",
    "    {\"name\": \"shipping_status\", \"sql\": \"SELECT order_id, status FROM shipments WHERE status = 'pending'\"},\n",
    "    {\"name\": \"returns_analysis\", \"sql\": \"SELECT product_id, COUNT(*) FROM returns GROUP BY product_id\"},\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"APPROACH 1: Manual Batching (3 queries at a time)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total queries: {len(sample_queries)}\")\n",
    "print(f\"Batch size: 3\")\n",
    "print(f\"Expected batches: {len(sample_queries) // 3 + (1 if len(sample_queries) % 3 else 0)}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Run the flow\n",
    "start_time = time.time()\n",
    "results = run_queries_in_batches(sample_queries, batch_size=3)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RESULTS:\")\n",
    "print(\"=\" * 70)\n",
    "for r in results:\n",
    "    print(f\"  • {r['query_name']}: {r['row_count']} rows in {r['execution_time']:.2f}s\")\n",
    "\n",
    "print(f\"\\n⏱️  Total execution time: {end_time - start_time:.2f} seconds\")\n",
    "print(f\"📊 Average time per query: {(end_time - start_time) / len(sample_queries):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec97820",
   "metadata": {},
   "source": [
    "## Approach 2: Using Task Concurrency Limits (Prefect 2.x Feature)\n",
    "\n",
    "This approach uses Prefect's built-in concurrency limiting to automatically control how many tasks run simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782cafb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prefect import flow, task\n",
    "\n",
    "# Define task with concurrency limit\n",
    "@task(\n",
    "    name=\"execute_sql_with_limit\",\n",
    "    tags=[\"database\"],\n",
    "    retries=2,\n",
    "    retry_delay_seconds=10\n",
    ")\n",
    "def execute_sql_with_concurrency_limit(query_name: str, query: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Execute SQL query with automatic concurrency limiting.\n",
    "    Prefect will ensure max 3 of these run at once if configured.\n",
    "    \"\"\"\n",
    "    print(f\"🔵 Starting query: {query_name} at {datetime.now().strftime('%H:%M:%S')}\")\n",
    "    \n",
    "    import random\n",
    "    execution_time = random.uniform(1, 3)\n",
    "    time.sleep(execution_time)\n",
    "    \n",
    "    result = {\n",
    "        \"query_name\": query_name,\n",
    "        \"query\": query,\n",
    "        \"row_count\": random.randint(100, 1000),\n",
    "        \"execution_time\": execution_time,\n",
    "        \"completed_at\": datetime.now().strftime('%H:%M:%S')\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ Completed query: {query_name} ({execution_time:.2f}s)\")\n",
    "    return result\n",
    "\n",
    "\n",
    "@flow(name=\"parallel_sql_with_concurrency_limit\")\n",
    "def run_queries_with_limit(queries: List[Dict[str, str]]):\n",
    "    \"\"\"\n",
    "    Submit all queries at once - Prefect controls concurrency automatically.\n",
    "    \n",
    "    Note: In production, set concurrency limit via:\n",
    "      1. Prefect UI: Settings → Concurrency Limits → Create\n",
    "      2. CLI: prefect concurrency-limit create database 3\n",
    "      3. Code: shown in next example\n",
    "    \"\"\"\n",
    "    # Submit ALL queries at once\n",
    "    futures = []\n",
    "    for q in queries:\n",
    "        future = execute_sql_with_concurrency_limit.submit(q[\"name\"], q[\"sql\"])\n",
    "        futures.append(future)\n",
    "    \n",
    "    # Prefect automatically limits concurrent execution to 3\n",
    "    # (if concurrency limit is configured)\n",
    "    results = [future.result() for future in futures]\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"APPROACH 2: Task Concurrency Limits\")\n",
    "print(\"=\" * 70)\n",
    "print(\"This approach submits ALL tasks at once.\")\n",
    "print(\"Prefect's concurrency limiter ensures only 3 run simultaneously.\")\n",
    "print()\n",
    "print(\"⚠️  Note: Concurrency limits must be configured on the Prefect server:\")\n",
    "print(\"   prefect concurrency-limit create database 3\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# For this demo, we'll use manual batching since concurrency limits\n",
    "# require server configuration. See next cell for how to set them up.\n",
    "print(\"\\n(Running with manual batching for demo purposes...)\")\n",
    "results = run_queries_in_batches(sample_queries, batch_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0416cdde",
   "metadata": {},
   "source": [
    "## Approach 3: Using Python's asyncio with Semaphore (Most Control)\n",
    "\n",
    "This approach gives you the most control over concurrency using Python's built-in asyncio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89354f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from prefect import flow, task\n",
    "\n",
    "@task(name=\"async_execute_sql\")\n",
    "async def execute_sql_async(query_name: str, query: str, semaphore: asyncio.Semaphore) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Async SQL execution with semaphore-based concurrency control.\n",
    "    The semaphore ensures max 3 queries run at once.\n",
    "    \"\"\"\n",
    "    async with semaphore:  # This blocks if 3 tasks are already running\n",
    "        print(f\"🔵 Starting query: {query_name} at {datetime.now().strftime('%H:%M:%S')}\")\n",
    "        \n",
    "        import random\n",
    "        execution_time = random.uniform(1, 3)\n",
    "        await asyncio.sleep(execution_time)  # Async sleep\n",
    "        \n",
    "        result = {\n",
    "            \"query_name\": query_name,\n",
    "            \"query\": query,\n",
    "            \"row_count\": random.randint(100, 1000),\n",
    "            \"execution_time\": execution_time,\n",
    "            \"completed_at\": datetime.now().strftime('%H:%M:%S')\n",
    "        }\n",
    "        \n",
    "        print(f\"✅ Completed query: {query_name} ({execution_time:.2f}s)\")\n",
    "        return result\n",
    "\n",
    "\n",
    "@flow(name=\"parallel_sql_with_semaphore\")\n",
    "async def run_queries_with_semaphore(queries: List[Dict[str, str]], max_concurrent: int = 3):\n",
    "    \"\"\"\n",
    "    Run SQL queries with semaphore-controlled concurrency.\n",
    "    \n",
    "    Args:\n",
    "        queries: List of query configurations\n",
    "        max_concurrent: Maximum number of concurrent queries (default: 3)\n",
    "    \"\"\"\n",
    "    # Create a semaphore that allows max_concurrent tasks at once\n",
    "    semaphore = asyncio.Semaphore(max_concurrent)\n",
    "    \n",
    "    # Create all tasks at once\n",
    "    tasks = [\n",
    "        execute_sql_async(q[\"name\"], q[\"sql\"], semaphore)\n",
    "        for q in queries\n",
    "    ]\n",
    "    \n",
    "    # Run all tasks - semaphore controls concurrency automatically\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"APPROACH 3: asyncio with Semaphore\")\n",
    "print(\"=\" * 70)\n",
    "print(\"This uses Python's asyncio.Semaphore to limit concurrency.\")\n",
    "print(\"Most flexible and works without Prefect server configuration.\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Run the async flow\n",
    "start_time = time.time()\n",
    "results = await run_queries_with_semaphore(sample_queries, max_concurrent=3)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RESULTS:\")\n",
    "print(\"=\" * 70)\n",
    "for r in results:\n",
    "    print(f\"  • {r['query_name']}: {r['row_count']} rows in {r['execution_time']:.2f}s\")\n",
    "\n",
    "print(f\"\\n⏱️  Total execution time: {end_time - start_time:.2f} seconds\")\n",
    "print(f\"📊 Queries completed: {len(results)}\")\n",
    "print(f\"🚀 Concurrency limit: 3 queries at a time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fa1109",
   "metadata": {},
   "source": [
    "## Real-World Example: Loading Queries from SQL Config File\n",
    "\n",
    "Here's a complete example showing how to read queries from a config file and run them in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0db0791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import json\n",
    "from pathlib import Path\n",
    "from prefect import flow, task\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Example 1: Reading from YAML config file\n",
    "sample_yaml_config = \"\"\"\n",
    "queries:\n",
    "  - name: daily_sales\n",
    "    sql: |\n",
    "      SELECT \n",
    "        date,\n",
    "        SUM(amount) as total_sales,\n",
    "        COUNT(*) as transaction_count\n",
    "      FROM sales\n",
    "      WHERE date >= CURRENT_DATE - INTERVAL '7 days'\n",
    "      GROUP BY date\n",
    "    \n",
    "  - name: top_products\n",
    "    sql: |\n",
    "      SELECT \n",
    "        product_id,\n",
    "        product_name,\n",
    "        SUM(quantity) as total_sold\n",
    "      FROM order_items\n",
    "      JOIN products USING (product_id)\n",
    "      WHERE order_date >= CURRENT_DATE - INTERVAL '30 days'\n",
    "      GROUP BY product_id, product_name\n",
    "      ORDER BY total_sold DESC\n",
    "      LIMIT 100\n",
    "    \n",
    "  - name: customer_segments\n",
    "    sql: |\n",
    "      SELECT \n",
    "        customer_segment,\n",
    "        COUNT(DISTINCT customer_id) as customer_count,\n",
    "        AVG(lifetime_value) as avg_ltv\n",
    "      FROM customers\n",
    "      GROUP BY customer_segment\n",
    "\"\"\"\n",
    "\n",
    "# Example 2: Reading from JSON config file\n",
    "sample_json_config = \"\"\"\n",
    "{\n",
    "  \"database\": {\n",
    "    \"host\": \"postgres.company.com\",\n",
    "    \"port\": 5432,\n",
    "    \"database\": \"analytics\"\n",
    "  },\n",
    "  \"queries\": [\n",
    "    {\n",
    "      \"name\": \"inventory_levels\",\n",
    "      \"sql\": \"SELECT warehouse_id, product_id, quantity FROM inventory WHERE quantity < reorder_level\",\n",
    "      \"priority\": \"high\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"pending_orders\",\n",
    "      \"sql\": \"SELECT order_id, customer_id, status, created_at FROM orders WHERE status = 'pending'\",\n",
    "      \"priority\": \"medium\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@task(name=\"load_queries_from_config\")\n",
    "def load_queries_from_file(config_path: str, format: str = \"yaml\") -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Load SQL queries from a configuration file.\n",
    "    \n",
    "    Args:\n",
    "        config_path: Path to the config file\n",
    "        format: File format ('yaml' or 'json')\n",
    "    \n",
    "    Returns:\n",
    "        List of query dictionaries with 'name' and 'sql' keys\n",
    "    \"\"\"\n",
    "    # In this demo, we'll use the sample configs\n",
    "    # In production, you'd read from actual files\n",
    "    \n",
    "    if format == \"yaml\":\n",
    "        config = yaml.safe_load(sample_yaml_config)\n",
    "    else:\n",
    "        config = json.loads(sample_json_config)\n",
    "    \n",
    "    queries = config.get(\"queries\", [])\n",
    "    \n",
    "    print(f\"📄 Loaded {len(queries)} queries from config file\")\n",
    "    for q in queries:\n",
    "        print(f\"   • {q['name']}\")\n",
    "    \n",
    "    return queries\n",
    "\n",
    "\n",
    "@task(name=\"execute_database_query\")\n",
    "def execute_database_query(query_name: str, query: str, db_config: Dict[str, Any] = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Execute a SQL query against a database.\n",
    "    \n",
    "    In production, this would:\n",
    "    1. Get connection from pool\n",
    "    2. Execute query\n",
    "    3. Fetch results\n",
    "    4. Return connection to pool\n",
    "    \"\"\"\n",
    "    print(f\"🔵 Executing: {query_name} at {datetime.now().strftime('%H:%M:%S')}\")\n",
    "    \n",
    "    # Simulate database query execution\n",
    "    import random\n",
    "    execution_time = random.uniform(1, 3)\n",
    "    time.sleep(execution_time)\n",
    "    \n",
    "    # Simulate results\n",
    "    result = {\n",
    "        \"query_name\": query_name,\n",
    "        \"success\": True,\n",
    "        \"row_count\": random.randint(50, 500),\n",
    "        \"execution_time\": execution_time,\n",
    "        \"data\": f\"[Sample data from {query_name}]\"\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ Completed: {query_name} - {result['row_count']} rows ({execution_time:.2f}s)\")\n",
    "    return result\n",
    "\n",
    "\n",
    "@flow(name=\"production_etl_pipeline\")\n",
    "def run_etl_pipeline(config_path: str = \"queries.yaml\", max_concurrent: int = 3):\n",
    "    \"\"\"\n",
    "    Production-ready ETL pipeline that:\n",
    "    1. Loads queries from config file\n",
    "    2. Executes them in parallel (max 3 at a time)\n",
    "    3. Collects and returns results\n",
    "    \n",
    "    Args:\n",
    "        config_path: Path to query config file\n",
    "        max_concurrent: Maximum concurrent queries\n",
    "    \"\"\"\n",
    "    # Step 1: Load queries from config\n",
    "    queries = load_queries_from_file(config_path, format=\"yaml\")\n",
    "    \n",
    "    # Step 2: Run queries in batches\n",
    "    all_results = []\n",
    "    \n",
    "    for i in range(0, len(queries), max_concurrent):\n",
    "        batch = queries[i:i + max_concurrent]\n",
    "        batch_num = i // max_concurrent + 1\n",
    "        \n",
    "        print(f\"\\n📦 Batch {batch_num}: Running {len(batch)} queries\")\n",
    "        \n",
    "        # Submit batch\n",
    "        futures = [\n",
    "            execute_database_query.submit(q[\"name\"], q[\"sql\"])\n",
    "            for q in batch\n",
    "        ]\n",
    "        \n",
    "        # Wait for completion\n",
    "        batch_results = [f.result() for f in futures]\n",
    "        all_results.extend(batch_results)\n",
    "    \n",
    "    # Step 3: Summary\n",
    "    successful = sum(1 for r in all_results if r[\"success\"])\n",
    "    total_rows = sum(r[\"row_count\"] for r in all_results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ETL PIPELINE SUMMARY:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"✅ Successful queries: {successful}/{len(all_results)}\")\n",
    "    print(f\"📊 Total rows processed: {total_rows:,}\")\n",
    "    print(f\"🚀 Concurrency: {max_concurrent} queries at a time\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "\n",
    "# Run the production pipeline\n",
    "print(\"=\" * 70)\n",
    "print(\"PRODUCTION EXAMPLE: ETL Pipeline with Config File\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "start_time = time.time()\n",
    "results = run_etl_pipeline(config_path=\"queries.yaml\", max_concurrent=3)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"\\n⏱️  Total pipeline time: {end_time - start_time:.2f} seconds\")\n",
    "print(f\"📈 Average query time: {(end_time - start_time) / len(results):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9280d8a",
   "metadata": {},
   "source": [
    "## Production Best Practices: Database Connection Pooling\n",
    "\n",
    "When running parallel SQL queries, use connection pooling to avoid overwhelming your database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334c5c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production example with connection pooling\n",
    "production_example = \"\"\"\n",
    "# ============================================================================\n",
    "# PRODUCTION CODE: Parallel SQL Queries with Connection Pooling\n",
    "# ============================================================================\n",
    "\n",
    "from prefect import flow, task\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.pool import QueuePool\n",
    "from typing import List, Dict, Any\n",
    "import pandas as pd\n",
    "\n",
    "# Global connection pool (created once, reused across tasks)\n",
    "# This is crucial for parallel execution!\n",
    "engine = create_engine(\n",
    "    \"postgresql://user:password@host:5432/database\",\n",
    "    poolclass=QueuePool,\n",
    "    pool_size=5,          # Base pool size\n",
    "    max_overflow=10,      # Can grow to 15 total connections\n",
    "    pool_pre_ping=True,   # Test connections before using\n",
    "    echo=False\n",
    ")\n",
    "\n",
    "\n",
    "@task(\n",
    "    name=\"execute_query_with_pool\",\n",
    "    retries=3,\n",
    "    retry_delay_seconds=10\n",
    ")\n",
    "def execute_query_with_pool(query_name: str, sql: str) -> pd.DataFrame:\n",
    "    \\\"\\\"\\\"\n",
    "    Execute SQL query using connection from pool.\n",
    "    \n",
    "    Connection pooling ensures:\n",
    "    - We don't create too many database connections\n",
    "    - Connections are reused efficiently\n",
    "    - Failed connections are recycled\n",
    "    \\\"\\\"\\\"\n",
    "    try:\n",
    "        # Get connection from pool\n",
    "        with engine.connect() as conn:\n",
    "            # Execute query\n",
    "            result = pd.read_sql(text(sql), conn)\n",
    "            \n",
    "            print(f\"✅ {query_name}: {len(result)} rows\")\n",
    "            return result\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ {query_name} failed: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "@flow(name=\"parallel_queries_production\")\n",
    "def run_parallel_queries(queries: List[Dict[str, str]], batch_size: int = 3):\n",
    "    \\\"\\\"\\\"\n",
    "    Run queries in parallel with connection pooling.\n",
    "    \n",
    "    The pool automatically limits concurrent connections,\n",
    "    so we can safely submit more tasks than we have connections.\n",
    "    \\\"\\\"\\\"\n",
    "    all_results = {}\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, len(queries), batch_size):\n",
    "        batch = queries[i:i + batch_size]\n",
    "        \n",
    "        # Submit batch (pool handles connection limiting)\n",
    "        futures = [\n",
    "            execute_query_with_pool.submit(q[\"name\"], q[\"sql\"])\n",
    "            for q in batch\n",
    "        ]\n",
    "        \n",
    "        # Collect results\n",
    "        for q, future in zip(batch, futures):\n",
    "            all_results[q[\"name\"]] = future.result()\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EXAMPLE USAGE\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load queries from config\n",
    "    import yaml\n",
    "    \n",
    "    with open(\"queries.yaml\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    # Run with 3 concurrent queries\n",
    "    results = run_parallel_queries(\n",
    "        queries=config[\"queries\"],\n",
    "        batch_size=3\n",
    "    )\n",
    "    \n",
    "    # Save results\n",
    "    for name, df in results.items():\n",
    "        df.to_parquet(f\"output/{name}.parquet\")\n",
    "        print(f\"Saved {name}: {len(df)} rows\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# KEY BENEFITS:\n",
    "# ============================================================================\n",
    "# \n",
    "# ✅ Connection Pooling:\n",
    "#    - Reuses database connections efficiently\n",
    "#    - Prevents connection exhaustion\n",
    "#    - Handles connection failures gracefully\n",
    "#\n",
    "# ✅ Parallel Execution:\n",
    "#    - Runs 3 queries at once\n",
    "#    - Reduces total pipeline time\n",
    "#    - Better resource utilization\n",
    "#\n",
    "# ✅ Error Handling:\n",
    "#    - Automatic retries (3 attempts)\n",
    "#    - Doesn't fail entire pipeline if one query fails\n",
    "#    - Detailed error logging\n",
    "#\n",
    "# ✅ Monitoring:\n",
    "#    - All queries tracked in Prefect UI\n",
    "#    - Can see which queries are slow\n",
    "#    - Full execution history\n",
    "#\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "\n",
    "print(production_example)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CONNECTION POOL CONFIGURATION GUIDE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "pooling_guide = \"\"\"\n",
    "When configuring connection pools for parallel queries:\n",
    "\n",
    "1. pool_size (base connections):\n",
    "   - Set to your typical concurrency level\n",
    "   - Example: If running 3 queries at a time → pool_size=3 to 5\n",
    "\n",
    "2. max_overflow (extra connections):\n",
    "   - Handles bursts when needed\n",
    "   - Example: max_overflow=5 allows up to 10 total (if pool_size=5)\n",
    "\n",
    "3. pool_pre_ping (connection testing):\n",
    "   - Always set to True for production\n",
    "   - Tests connections before use to avoid stale connection errors\n",
    "\n",
    "4. Database limits:\n",
    "   - Check your database's max_connections setting\n",
    "   - Leave headroom for other applications\n",
    "   - Example: DB max=100, use pool_size=5 + max_overflow=10 per worker\n",
    "\n",
    "5. Multiple workers:\n",
    "   - If running 3 workers, each needs its own pool\n",
    "   - Total connections = workers × (pool_size + max_overflow)\n",
    "   - Example: 3 workers × 15 max = 45 total connections\n",
    "\n",
    "RECOMMENDED SETTINGS FOR PARALLEL QUERIES:\n",
    "\n",
    "# For 3 concurrent queries per worker:\n",
    "engine = create_engine(\n",
    "    connection_string,\n",
    "    poolclass=QueuePool,\n",
    "    pool_size=3,              # Match concurrency\n",
    "    max_overflow=2,           # Allow bursts\n",
    "    pool_pre_ping=True,       # Test connections\n",
    "    pool_recycle=3600,        # Recycle connections every hour\n",
    "    echo=False                # Set True for debugging\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "print(pooling_guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fc1f30",
   "metadata": {},
   "source": [
    "## Summary: Comparison of Approaches\n",
    "\n",
    "Let's compare the three approaches and help you choose the right one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb58e94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create comparison table\n",
    "comparison_data = {\n",
    "    \"Approach\": [\n",
    "        \"1. Manual Batching\\n(.submit() + batching)\",\n",
    "        \"2. Concurrency Limits\\n(Prefect server config)\",\n",
    "        \"3. asyncio Semaphore\\n(Python native)\"\n",
    "    ],\n",
    "    \"Pros\": [\n",
    "        \"• Simple to understand\\n• No server config needed\\n• Works immediately\\n• Full control over batching\",\n",
    "        \"• Clean code\\n• Centralized control\\n• Server enforces limits\\n• Works across deployments\",\n",
    "        \"• Most flexible\\n• No dependencies\\n• Great for async code\\n• Fine-grained control\"\n",
    "    ],\n",
    "    \"Cons\": [\n",
    "        \"• Manual batch logic\\n• More verbose code\\n• Harder to change limit\",\n",
    "        \"• Requires server setup\\n• Must configure limits first\\n• Less obvious in code\",\n",
    "        \"• Requires async/await\\n• More complex syntax\\n• Need to understand asyncio\"\n",
    "    ],\n",
    "    \"Best For\": [\n",
    "        \"• Quick prototypes\\n• Simple pipelines\\n• Learning Prefect\\n• No server access\",\n",
    "        \"• Production systems\\n• Multiple deployments\\n• Team workflows\\n• Centralized governance\",\n",
    "        \"• Complex async flows\\n• Maximum performance\\n• I/O-bound operations\\n• Advanced users\"\n",
    "    ],\n",
    "    \"Recommended?\": [\n",
    "        \"✅ Start here\",\n",
    "        \"🚀 Production\",\n",
    "        \"⚡ Advanced\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"COMPARISON OF PARALLEL EXECUTION APPROACHES\")\n",
    "print(\"=\" * 100)\n",
    "print()\n",
    "for idx, row in df.iterrows():\n",
    "    print(f\"{'='*100}\")\n",
    "    print(f\"{row['Approach']} - {row['Recommended?']}\")\n",
    "    print(f\"{'='*100}\")\n",
    "    print(f\"\\n✅ PROS:\\n{row['Pros']}\")\n",
    "    print(f\"\\n❌ CONS:\\n{row['Cons']}\")\n",
    "    print(f\"\\n🎯 BEST FOR:\\n{row['Best For']}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"RECOMMENDATIONS:\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "recommendations = \"\"\"\n",
    "📝 For Your Use Case (SQL queries from config file):\n",
    "\n",
    "1️⃣ START WITH APPROACH 1 (Manual Batching):\n",
    "   - Easy to implement right now\n",
    "   - No extra setup required\n",
    "   - Perfect for getting started\n",
    "   - Code example provided above\n",
    "\n",
    "2️⃣ MOVE TO APPROACH 2 (Concurrency Limits) when:\n",
    "   - You have Prefect server running\n",
    "   - Multiple people on the team\n",
    "   - Want centralized control\n",
    "   - Running in production\n",
    "\n",
    "3️⃣ CONSIDER APPROACH 3 (Semaphore) if:\n",
    "   - You need maximum performance\n",
    "   - Already using async/await\n",
    "   - Have complex async operations\n",
    "   - Comfortable with asyncio\n",
    "\n",
    "\n",
    "🏆 RECOMMENDED IMPLEMENTATION FOR YOU:\n",
    "\n",
    "from prefect import flow, task\n",
    "from typing import List, Dict\n",
    "import yaml\n",
    "\n",
    "@task(name=\"execute_sql\", retries=2)\n",
    "def execute_sql(query_name: str, sql: str) -> dict:\n",
    "    # Your database query logic here\n",
    "    with db_connection_pool.connect() as conn:\n",
    "        result = pd.read_sql(sql, conn)\n",
    "        return {\"name\": query_name, \"rows\": len(result), \"data\": result}\n",
    "\n",
    "@flow(name=\"parallel_etl\")\n",
    "def run_etl(config_file: str, batch_size: int = 3):\n",
    "    # Load queries from config\n",
    "    with open(config_file) as f:\n",
    "        queries = yaml.safe_load(f)[\"queries\"]\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    # Process in batches of 3\n",
    "    for i in range(0, len(queries), batch_size):\n",
    "        batch = queries[i:i + batch_size]\n",
    "        \n",
    "        # Submit all queries in this batch\n",
    "        futures = [execute_sql.submit(q[\"name\"], q[\"sql\"]) for q in batch]\n",
    "        \n",
    "        # Wait for batch to complete\n",
    "        batch_results = [f.result() for f in futures]\n",
    "        all_results.extend(batch_results)\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Usage:\n",
    "results = run_etl(\"queries.yaml\", batch_size=3)\n",
    "\"\"\"\n",
    "\n",
    "print(recommendations)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "perf_example = \"\"\"\n",
    "Example: 10 queries, each takes 2 seconds\n",
    "\n",
    "Sequential (old way):\n",
    "  • Query 1: 2s\n",
    "  • Query 2: 2s\n",
    "  • ... (8 more)\n",
    "  • Query 10: 2s\n",
    "  TOTAL: 20 seconds ❌\n",
    "\n",
    "Parallel with batch_size=3 (new way):\n",
    "  • Batch 1 (queries 1-3): 2s (all run together)\n",
    "  • Batch 2 (queries 4-6): 2s (all run together)\n",
    "  • Batch 3 (queries 7-9): 2s (all run together)\n",
    "  • Batch 4 (query 10): 2s\n",
    "  TOTAL: 8 seconds ✅ (2.5x faster!)\n",
    "\n",
    "Parallel with batch_size=5:\n",
    "  • Batch 1 (queries 1-5): 2s\n",
    "  • Batch 2 (queries 6-10): 2s\n",
    "  TOTAL: 4 seconds ✅ (5x faster!)\n",
    "\n",
    "⚠️  BUT: More concurrency = more database load\n",
    "    Always check your database can handle it!\n",
    "\"\"\"\n",
    "\n",
    "print(perf_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c3cc49",
   "metadata": {},
   "source": [
    "## 🎯 Quick Start: Your Exact Use Case\n",
    "\n",
    "Here's the complete code you can copy and adapt for your SQL config file scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb797d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_solution = \"\"\"\n",
    "# ============================================================================\n",
    "# COMPLETE SOLUTION: Parallel SQL Queries from Config File\n",
    "# ============================================================================\n",
    "# \n",
    "# This is ready-to-use code that you can adapt to your project.\n",
    "# It runs 3 SQL queries at a time instead of 1.\n",
    "#\n",
    "\n",
    "from prefect import flow, task\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.pool import QueuePool\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: Database Connection Pool Setup\n",
    "# ============================================================================\n",
    "\n",
    "def get_db_engine():\n",
    "    \\\"\\\"\\\"\n",
    "    Create database engine with connection pooling.\n",
    "    Modify the connection string for your database.\n",
    "    \\\"\\\"\\\"\n",
    "    return create_engine(\n",
    "        # Replace with your database connection string:\n",
    "        \"postgresql://user:password@host:5432/database\",\n",
    "        # Connection pool settings:\n",
    "        poolclass=QueuePool,\n",
    "        pool_size=3,           # Match your concurrency level\n",
    "        max_overflow=2,        # Allow some burst capacity\n",
    "        pool_pre_ping=True,    # Test connections before use\n",
    "        pool_recycle=3600,     # Recycle connections hourly\n",
    "        echo=False             # Set True for SQL logging\n",
    "    )\n",
    "\n",
    "\n",
    "# Global engine (created once, reused)\n",
    "engine = get_db_engine()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: Task to Execute Individual Queries\n",
    "# ============================================================================\n",
    "\n",
    "@task(\n",
    "    name=\"execute_single_query\",\n",
    "    retries=2,\n",
    "    retry_delay_seconds=10,\n",
    "    tags=[\"database\", \"sql\"]\n",
    ")\n",
    "def execute_query(query_name: str, sql: str) -> Dict[str, Any]:\n",
    "    \\\"\\\"\\\"\n",
    "    Execute a single SQL query and return results.\n",
    "    \n",
    "    Args:\n",
    "        query_name: Name/identifier for the query\n",
    "        sql: SQL query string to execute\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with query results and metadata\n",
    "    \\\"\\\"\\\"\n",
    "    try:\n",
    "        # Get connection from pool\n",
    "        with engine.connect() as conn:\n",
    "            # Execute query and load into DataFrame\n",
    "            df = pd.read_sql(text(sql), conn)\n",
    "            \n",
    "            result = {\n",
    "                \"name\": query_name,\n",
    "                \"success\": True,\n",
    "                \"row_count\": len(df),\n",
    "                \"columns\": list(df.columns),\n",
    "                \"data\": df,\n",
    "                \"error\": None\n",
    "            }\n",
    "            \n",
    "            print(f\"✅ {query_name}: Retrieved {len(df)} rows\")\n",
    "            return result\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ {query_name} failed: {str(e)}\")\n",
    "        return {\n",
    "            \"name\": query_name,\n",
    "            \"success\": False,\n",
    "            \"row_count\": 0,\n",
    "            \"columns\": [],\n",
    "            \"data\": None,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: Load Queries from Config File\n",
    "# ============================================================================\n",
    "\n",
    "@task(name=\"load_query_config\")\n",
    "def load_queries(config_path: str) -> List[Dict[str, str]]:\n",
    "    \\\"\\\"\\\"\n",
    "    Load SQL queries from YAML or JSON config file.\n",
    "    \n",
    "    Expected format (YAML):\n",
    "    queries:\n",
    "      - name: query1\n",
    "        sql: SELECT * FROM table1\n",
    "      - name: query2\n",
    "        sql: SELECT * FROM table2\n",
    "    \n",
    "    Args:\n",
    "        config_path: Path to config file\n",
    "    \n",
    "    Returns:\n",
    "        List of query dictionaries\n",
    "    \\\"\\\"\\\"\n",
    "    config_file = Path(config_path)\n",
    "    \n",
    "    if config_file.suffix in ['.yaml', '.yml']:\n",
    "        with open(config_file) as f:\n",
    "            config = yaml.safe_load(f)\n",
    "    elif config_file.suffix == '.json':\n",
    "        import json\n",
    "        with open(config_file) as f:\n",
    "            config = json.load(f)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {config_file.suffix}\")\n",
    "    \n",
    "    queries = config.get(\"queries\", [])\n",
    "    print(f\"📄 Loaded {len(queries)} queries from {config_path}\")\n",
    "    \n",
    "    return queries\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: Main Flow - Run Queries in Parallel (3 at a time)\n",
    "# ============================================================================\n",
    "\n",
    "@flow(name=\"parallel_sql_pipeline\")\n",
    "def run_parallel_queries(\n",
    "    config_path: str,\n",
    "    batch_size: int = 3,\n",
    "    output_dir: str = \"output\"\n",
    ") -> Dict[str, Any]:\n",
    "    \\\"\\\"\\\"\n",
    "    Main flow that executes SQL queries in parallel.\n",
    "    \n",
    "    Args:\n",
    "        config_path: Path to query configuration file\n",
    "        batch_size: Number of queries to run concurrently (default: 3)\n",
    "        output_dir: Directory to save results (optional)\n",
    "    \n",
    "    Returns:\n",
    "        Summary of execution results\n",
    "    \\\"\\\"\\\"\n",
    "    # Load queries from config file\n",
    "    queries = load_queries(config_path)\n",
    "    \n",
    "    if not queries:\n",
    "        print(\"⚠️  No queries found in config file\")\n",
    "        return {\"success\": False, \"results\": []}\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    # Process queries in batches\n",
    "    total_batches = (len(queries) + batch_size - 1) // batch_size\n",
    "    \n",
    "    for i in range(0, len(queries), batch_size):\n",
    "        batch = queries[i:i + batch_size]\n",
    "        batch_num = i // batch_size + 1\n",
    "        \n",
    "        print(f\"\\\\n{'='*70}\")\n",
    "        print(f\"📦 Batch {batch_num}/{total_batches}: Running {len(batch)} queries\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Submit all queries in this batch (runs in parallel)\n",
    "        futures = [\n",
    "            execute_query.submit(q[\"name\"], q[\"sql\"])\n",
    "            for q in batch\n",
    "        ]\n",
    "        \n",
    "        # Wait for all queries in batch to complete\n",
    "        batch_results = [future.result() for future in futures]\n",
    "        all_results.extend(batch_results)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Optional: Save results to files\n",
    "    # ========================================================================\n",
    "    if output_dir:\n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(exist_ok=True)\n",
    "        \n",
    "        for result in all_results:\n",
    "            if result[\"success\"] and result[\"data\"] is not None:\n",
    "                filename = output_path / f\"{result['name']}.parquet\"\n",
    "                result[\"data\"].to_parquet(filename)\n",
    "                print(f\"💾 Saved {result['name']} to {filename}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Summary\n",
    "    # ========================================================================\n",
    "    successful = sum(1 for r in all_results if r[\"success\"])\n",
    "    failed = len(all_results) - successful\n",
    "    total_rows = sum(r[\"row_count\"] for r in all_results if r[\"success\"])\n",
    "    \n",
    "    print(f\"\\\\n{'='*70}\")\n",
    "    print(\"📊 PIPELINE SUMMARY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"✅ Successful queries: {successful}/{len(all_results)}\")\n",
    "    print(f\"❌ Failed queries: {failed}\")\n",
    "    print(f\"📈 Total rows retrieved: {total_rows:,}\")\n",
    "    print(f\"🚀 Concurrency: {batch_size} queries at a time\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    return {\n",
    "        \"success\": failed == 0,\n",
    "        \"total_queries\": len(all_results),\n",
    "        \"successful\": successful,\n",
    "        \"failed\": failed,\n",
    "        \"total_rows\": total_rows,\n",
    "        \"results\": all_results\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# USAGE EXAMPLE\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the pipeline\n",
    "    summary = run_parallel_queries(\n",
    "        config_path=\"queries.yaml\",\n",
    "        batch_size=3,\n",
    "        output_dir=\"query_results\"\n",
    "    )\n",
    "    \n",
    "    # Check results\n",
    "    if summary[\"success\"]:\n",
    "        print(f\"\\\\n🎉 All {summary['successful']} queries completed successfully!\")\n",
    "    else:\n",
    "        print(f\"\\\\n⚠️  {summary['failed']} queries failed. Check logs for details.\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EXAMPLE CONFIG FILE (queries.yaml)\n",
    "# ============================================================================\n",
    "#\n",
    "# queries:\n",
    "#   - name: daily_sales\n",
    "#     sql: |\n",
    "#       SELECT \n",
    "#         date,\n",
    "#         SUM(amount) as total_sales\n",
    "#       FROM sales\n",
    "#       WHERE date >= CURRENT_DATE - INTERVAL '7 days'\n",
    "#       GROUP BY date\n",
    "#   \n",
    "#   - name: top_products\n",
    "#     sql: |\n",
    "#       SELECT \n",
    "#         product_id,\n",
    "#         COUNT(*) as order_count\n",
    "#       FROM orders\n",
    "#       GROUP BY product_id\n",
    "#       ORDER BY order_count DESC\n",
    "#       LIMIT 100\n",
    "#\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "\n",
    "print(complete_solution)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"🚀 TO USE THIS IN YOUR PROJECT:\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "usage_steps = \"\"\"\n",
    "1. Create queries.yaml with your SQL queries:\n",
    "   \n",
    "   queries:\n",
    "     - name: my_query_1\n",
    "       sql: SELECT * FROM table1\n",
    "     - name: my_query_2\n",
    "       sql: SELECT * FROM table2\n",
    "     # ... add more queries\n",
    "\n",
    "2. Update database connection string in get_db_engine():\n",
    "   \n",
    "   \"postgresql://user:password@host:5432/database\"\n",
    "   # Or use environment variables:\n",
    "   import os\n",
    "   os.getenv(\"DATABASE_URL\")\n",
    "\n",
    "3. Run the flow:\n",
    "   \n",
    "   python your_script.py\n",
    "   \n",
    "   # Or deploy it:\n",
    "   prefect deploy your_script.py:run_parallel_queries\n",
    "\n",
    "4. Monitor in Prefect UI:\n",
    "   \n",
    "   http://localhost:4200\n",
    "   You'll see each query as a separate task!\n",
    "\n",
    "5. Adjust batch_size based on:\n",
    "   \n",
    "   • Your database capacity (check max_connections)\n",
    "   • Query complexity (simple queries = higher batch_size)\n",
    "   • Available resources (memory, CPU)\n",
    "   \n",
    "   Start with 3, then test 5, 10, etc.\n",
    "\"\"\"\n",
    "\n",
    "print(usage_steps)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"✅ YOU'RE READY! Copy the code above and customize for your needs.\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28690390",
   "metadata": {},
   "source": [
    "# Learning Prefect: Hello World Flow\n",
    "\n",
    "This notebook will teach you the basics of Prefect by creating a simple \"Hello World\" flow.\n",
    "\n",
    "**What you'll learn:**\n",
    "1. How to create a Prefect `@task` - a unit of work\n",
    "2. How to create a Prefect `@flow` - orchestrates tasks\n",
    "3. How to run flows and see them in the Prefect UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac94ef7b",
   "metadata": {},
   "source": [
    "## Step 1: Import Prefect and configure the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29eedd6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Prefect imported and configured to use: http://localhost:4200/api\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from prefect import flow, task\n",
    "\n",
    "# Configure to use your local Prefect server\n",
    "os.environ[\"PREFECT_API_URL\"] = \"http://localhost:4200/api\"\n",
    "\n",
    "print(\"✅ Prefect imported and configured to use:\", os.environ[\"PREFECT_API_URL\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b08dbf",
   "metadata": {},
   "source": [
    "## Step 2: Create a simple task\n",
    "\n",
    "A **task** is a Python function decorated with `@task`. Tasks are the building blocks of flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a6ab7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Task 'print_hello' created successfully!\n",
      "Note: Tasks can only be called from within flows\n"
     ]
    }
   ],
   "source": [
    "@task\n",
    "def print_hello():\n",
    "    \"\"\"A simple task that prints Hello World\"\"\"\n",
    "    message = \"Hello World from Prefect!\"\n",
    "    print(message)\n",
    "    return message\n",
    "\n",
    "# Note: Tasks must be called from within a flow!\n",
    "# If you want to test the function directly, use: print_hello.fn()\n",
    "print(\"✅ Task 'print_hello' created successfully!\")\n",
    "print(\"Note: Tasks can only be called from within flows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b23f44",
   "metadata": {},
   "source": [
    "## Step 3: Create a flow that uses the task\n",
    "\n",
    "A **flow** is the main orchestrator. It calls tasks and manages the workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c05be1",
   "metadata": {},
   "source": [
    "### First, let's verify connection to the Prefect server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "673e97cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully connected to Prefect server!\n",
      "Server response: True\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Test connection to Prefect server\n",
    "try:\n",
    "    response = requests.get(\"http://localhost:4200/api/health\")\n",
    "    if response.status_code == 200:\n",
    "        print(\"✅ Successfully connected to Prefect server!\")\n",
    "        print(f\"Server response: {response.json()}\")\n",
    "    else:\n",
    "        print(f\"❌ Server returned status code: {response.status_code}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Could not connect to server: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199d5c67",
   "metadata": {},
   "source": [
    "### Now let's run a simple flow!\n",
    "\n",
    "For this to work with the external Prefect server, we'll write the flow to a Python file and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0cba5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flow Output:\n",
      "What is your favorite number?\n",
      "Favorite number: 42\n",
      "Customer IDs: ['customer30', 'customer71', 'customer27', 'customer56', 'customer72', 'customer95', 'customer47', 'customer75', 'customer71', 'customer28']\n",
      "\n",
      "\n",
      "Warnings/Errors:\n",
      "WARNING: Active profile 'local' set in the profiles file not found. The default profile will be used instead. \n",
      "19:18:10.244 | \u001b[36mINFO\u001b[0m    | prefect.engine - Created flow run\u001b[35m 'organic-spoonbill'\u001b[0m for flow\u001b[1;35m 'my-favorite-function'\u001b[0m\n",
      "19:18:10.245 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'organic-spoonbill'\u001b[0m - View at \u001b[94mhttp://localhost:4200/flow-runs/flow-run/eddf86db-c258-473a-b62d-c85f973ed21b\u001b[0m\n",
      "19:18:10.327 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'organic-spoonbill'\u001b[0m - Created task run 'get_customer_ids-0' for task 'get_customer_ids'\n",
      "19:18:10.328 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'organic-spoonbill'\u001b[0m - Executing 'get_customer_ids-0' immediately...\n",
      "19:18:10.400 | \u001b[36mINFO\u001b[0m    | Task run 'get_customer_ids-0' - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "19:18:10.418 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'organic-spoonbill'\u001b[0m - Finished in state \u001b[32mCompleted\u001b[0m()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's use the existing prefect-main.py instead\n",
    "# First, let's run it!\n",
    "import subprocess\n",
    "\n",
    "result = subprocess.run(\n",
    "    [\"uv\", \"run\", \"python\", \"prefect-main.py\"],\n",
    "    cwd=\"/Users/jichong/projects/playground/BOS/prefect-poc\",\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    "    env={**os.environ, \"PREFECT_API_URL\": \"http://localhost:4200/api\"}\n",
    ")\n",
    "\n",
    "print(\"Flow Output:\")\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"\\nWarnings/Errors:\")\n",
    "    print(result.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac11a71",
   "metadata": {},
   "source": [
    "### Check the Prefect UI!\n",
    "\n",
    "🎉 **Success!** Your flow ran and was tracked by the Prefect server.\n",
    "\n",
    "**What just happened:**\n",
    "1. The flow `my-favorite-function` executed successfully\n",
    "2. It created and executed the task `get_customer_ids`\n",
    "3. All metadata was sent to your Prefect server at http://localhost:4200\n",
    "4. You can see the run at the URL shown above (http://localhost:4200/flow-runs/flow-run/...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce4a157",
   "metadata": {},
   "source": [
    "### Query flow runs from the Prefect API\n",
    "\n",
    "Now let's see how to programmatically query the flows and flow runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aab3d074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 recent flow runs:\n",
      "\n",
      "  • Flow: organic-spoonbill\n",
      "    State: COMPLETED\n",
      "    Start time: 2025-10-14T11:18:10.248871+00:00\n",
      "    ID: eddf86db-c258-473a-b62d-c85f973ed21b\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from prefect.client.orchestration import PrefectClient\n",
    "\n",
    "async def get_recent_flow_runs():\n",
    "    \"\"\"Query recent flow runs from the Prefect server\"\"\"\n",
    "    async with PrefectClient(api=\"http://localhost:4200/api\") as client:\n",
    "        # Get recent flow runs\n",
    "        runs = await client.read_flow_runs(limit=5)\n",
    "        \n",
    "        print(f\"Found {len(runs)} recent flow runs:\\n\")\n",
    "        for run in runs:\n",
    "            print(f\"  • Flow: {run.name}\")\n",
    "            print(f\"    State: {run.state.type if run.state else 'Unknown'}\")\n",
    "            print(f\"    Start time: {run.start_time}\")\n",
    "            print(f\"    ID: {run.id}\")\n",
    "            print()\n",
    "        \n",
    "        return runs\n",
    "\n",
    "# Run the async function\n",
    "runs = await get_recent_flow_runs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eea6c9a",
   "metadata": {},
   "source": [
    "# Part 2: Deployments and Work Pools\n",
    "\n",
    "Now let's learn about **Deployments** and **Work Pools** - how to let the Prefect server schedule and orchestrate your flows!\n",
    "\n",
    "## Concepts:\n",
    "\n",
    "- **Deployment**: Packages your flow for remote execution and scheduling\n",
    "- **Work Pool**: A queue where the server sends flow runs to be executed\n",
    "- **Worker**: An agent that pulls work from the pool and executes it\n",
    "\n",
    "Think of it like a restaurant:\n",
    "- **Deployment** = Menu item (your flow, ready to be ordered)\n",
    "- **Work Pool** = Kitchen ticket queue\n",
    "- **Worker** = Chef who takes tickets and cooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e36361",
   "metadata": {},
   "source": [
    "## Step 1: Create a Work Pool\n",
    "\n",
    "First, we need to create a work pool. A work pool is like a job queue where the server puts flow runs that need to be executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23d2e04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Work Pool Creation:\n",
      "\u001b[32mCreated work pool 'my-local-pool'.\u001b[0m\n",
      "\n",
      "WARNING: Active profile 'local' set in the profiles file not found. The default profile will be used instead. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a work pool using the Prefect CLI\n",
    "import subprocess\n",
    "\n",
    "# Create a process work pool (runs flows as local processes)\n",
    "result = subprocess.run(\n",
    "    [\"uv\", \"run\", \"prefect\", \"work-pool\", \"create\", \"my-local-pool\", \"--type\", \"process\"],\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    "    env={**os.environ, \"PREFECT_API_URL\": \"http://localhost:4200/api\"}\n",
    ")\n",
    "\n",
    "print(\"Work Pool Creation:\")\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(result.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650eec67",
   "metadata": {},
   "source": [
    "## Step 2: Create a Deployment\n",
    "\n",
    "Now let's deploy our flow. This tells the Prefect server about the flow and how to run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99aeb9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Deployment Creation Guide\n",
      "======================================================================\n",
      "\n",
      "To create the deployment, run these commands in your terminal:\n",
      "\n",
      "cd /Users/jichong/projects/playground/BOS/prefect-poc\n",
      "export PREFECT_API_URL=http://localhost:4200/api\n",
      "\n",
      "# Method 1: Quick deploy (interactive)\n",
      "uv run prefect deploy deployed_flow.py:data_pipeline \\\n",
      "    --name my-data-pipeline \\\n",
      "    --pool my-local-pool\n",
      "\n",
      "# Method 2: Or use Python code\n",
      "\n",
      "\n",
      "✅ Work pools available on server:\n",
      "   • default-agent-pool (type: prefect-agent)\n",
      "   • my-local-pool (type: process)\n",
      "\n",
      "💡 Next step: Start a worker (see next cell)\n"
     ]
    }
   ],
   "source": [
    "# For demonstration, let's show how to create a deployment via the command line\n",
    "# This is the recommended approach for Prefect 2.13\n",
    "\n",
    "deployment_guide = \"\"\"\n",
    "To create the deployment, run these commands in your terminal:\n",
    "\n",
    "cd /Users/jichong/projects/playground/BOS/prefect-poc\n",
    "export PREFECT_API_URL=http://localhost:4200/api\n",
    "\n",
    "# Method 1: Quick deploy (interactive)\n",
    "uv run prefect deploy deployed_flow.py:data_pipeline \\\\\n",
    "    --name my-data-pipeline \\\\\n",
    "    --pool my-local-pool\n",
    "\n",
    "# Method 2: Or use Python code\n",
    "\"\"\"\n",
    "\n",
    "print(\"📋 Deployment Creation Guide\")\n",
    "print(\"=\" * 70)\n",
    "print(deployment_guide)\n",
    "\n",
    "# For this demo, let's verify the work pool exists and is ready\n",
    "async def check_work_pool():\n",
    "    async with PrefectClient(api=\"http://localhost:4200/api\") as client:\n",
    "        try:\n",
    "            pools = await client.read_work_pools()\n",
    "            print(f\"\\n✅ Work pools available on server:\")\n",
    "            for pool in pools:\n",
    "                print(f\"   • {pool.name} (type: {pool.type})\")\n",
    "            return pools\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error checking work pools: {e}\")\n",
    "            return []\n",
    "\n",
    "pools = await check_work_pool()\n",
    "\n",
    "print(\"\\n💡 Next step: Start a worker (see next cell)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6159418",
   "metadata": {},
   "source": [
    "## Step 3: Start a Worker\n",
    "\n",
    "A worker is the \"chef\" that pulls jobs from the work pool and executes them. Without a worker, deployments won't run!\n",
    "\n",
    "**Note:** We'll start the worker in the background so it can pick up flow runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70db5c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 To start a worker, run this command in a SEPARATE TERMINAL:\n",
      "\n",
      "# Run this in a separate terminal:\n",
      "cd /Users/jichong/projects/playground/BOS/prefect-poc\n",
      "export PREFECT_API_URL=http://localhost:4200/api\n",
      "uv run prefect worker start --pool my-local-pool\n",
      "\n",
      "\n",
      "The worker will:\n",
      "  1. Connect to the Prefect server\n",
      "  2. Poll the 'my-local-pool' work pool for flow runs\n",
      "  3. Execute any flows that are scheduled\n",
      "  4. Report results back to the server\n",
      "\n",
      "⚠️  Leave the worker running to process flows!\n"
     ]
    }
   ],
   "source": [
    "# We'll show you the command to start a worker\n",
    "# In practice, you'd run this in a separate terminal\n",
    "\n",
    "worker_command = \"\"\"\n",
    "# Run this in a separate terminal:\n",
    "cd /Users/jichong/projects/playground/BOS/prefect-poc\n",
    "export PREFECT_API_URL=http://localhost:4200/api\n",
    "uv run prefect worker start --pool my-local-pool\n",
    "\"\"\"\n",
    "\n",
    "print(\"🔧 To start a worker, run this command in a SEPARATE TERMINAL:\")\n",
    "print(worker_command)\n",
    "print(\"\\nThe worker will:\")\n",
    "print(\"  1. Connect to the Prefect server\")\n",
    "print(\"  2. Poll the 'my-local-pool' work pool for flow runs\")\n",
    "print(\"  3. Execute any flows that are scheduled\")\n",
    "print(\"  4. Report results back to the server\")\n",
    "print(\"\\n⚠️  Leave the worker running to process flows!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6073e19e",
   "metadata": {},
   "source": [
    "## Step 4: Trigger a Flow Run from the Server\n",
    "\n",
    "Now that we have a deployment, we can trigger it through the server API (not by running the Python file directly!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71cc4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigger a flow run via the API\n",
    "# This creates a flow run that gets queued in the work pool\n",
    "\n",
    "result = subprocess.run([\n",
    "    \"uv\", \"run\", \"prefect\", \"deployment\", \"run\",\n",
    "    \"data-pipeline/my-data-pipeline\",  # flow-name/deployment-name\n",
    "    \"--param\", \"source=production-api\",\n",
    "    \"--param\", \"destination=warehouse\"\n",
    "],\n",
    "    cwd=\"/Users/jichong/projects/playground/BOS/prefect-poc\",\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    "    env={**os.environ, \"PREFECT_API_URL\": \"http://localhost:4200/api\"}\n",
    ")\n",
    "\n",
    "print(\"Flow Run Triggered!\")\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"\\nMessages:\")\n",
    "    print(result.stderr)\n",
    "    \n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"What happens now:\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. ✅ Server creates a flow run and puts it in the work pool queue\")\n",
    "print(\"2. ⏳ Worker (if running) picks up the flow run\")\n",
    "print(\"3. 🚀 Worker executes the flow on its machine\")\n",
    "print(\"4. 📊 Worker reports progress back to the server\")\n",
    "print(\"5. 🎉 You can watch it in the UI at http://localhost:4200\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04399544",
   "metadata": {},
   "source": [
    "## Summary: How Deployments Work\n",
    "\n",
    "### The Full Architecture:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────┐\n",
    "│  YOU (Developer)                                    │\n",
    "│                                                     │\n",
    "│  1. Create flow (deployed_flow.py)                 │\n",
    "│  2. Deploy it: prefect deploy ...                  │\n",
    "│     → Tells server about the flow                  │\n",
    "│     → Server stores deployment metadata            │\n",
    "└─────────────────────────────────────────────────────┘\n",
    "                    │\n",
    "                    │ deployment info\n",
    "                    ▼\n",
    "┌─────────────────────────────────────────────────────┐\n",
    "│  PREFECT SERVER (Docker at :4200)                  │\n",
    "│                                                     │\n",
    "│  • Stores deployment definitions                   │\n",
    "│  • Maintains work pool queues                      │\n",
    "│  • Accepts flow run requests                       │\n",
    "│  • Tracks all execution metadata                   │\n",
    "└─────────────────────────────────────────────────────┘\n",
    "                    │\n",
    "                    │ flow runs queued here\n",
    "                    ▼\n",
    "┌─────────────────────────────────────────────────────┐\n",
    "│  WORK POOL: my-local-pool                          │\n",
    "│                                                     │\n",
    "│  Queue of pending flow runs waiting to execute     │\n",
    "└─────────────────────────────────────────────────────┘\n",
    "                    │\n",
    "                    │ worker polls for work\n",
    "                    ▼\n",
    "┌─────────────────────────────────────────────────────┐\n",
    "│  WORKER (prefect worker start --pool ...)          │\n",
    "│                                                     │\n",
    "│  • Polls work pool every few seconds               │\n",
    "│  • Picks up flow runs from queue                   │\n",
    "│  • Executes the flow on its machine                │\n",
    "│  • Sends progress/logs back to server              │\n",
    "└─────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Key Differences from Before:\n",
    "\n",
    "| Aspect | Direct Execution | With Deployment |\n",
    "|--------|-----------------|-----------------|\n",
    "| **How to run** | `python flow.py` | Server API or schedule |\n",
    "| **Who triggers** | You manually | Server (on schedule or API call) |\n",
    "| **Where code runs** | Your terminal | Worker machine |\n",
    "| **Code location** | Must be local | Must be accessible to worker |\n",
    "| **Scheduling** | Manual only | Can schedule (cron, interval, etc.) |\n",
    "| **Production ready** | No | Yes |\n",
    "\n",
    "### Use Cases:\n",
    "\n",
    "- **Direct execution** (what we did first): Development, testing, one-off runs\n",
    "- **Deployments + Workers**: Production, scheduled jobs, distributed execution, team workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823822b5",
   "metadata": {},
   "source": [
    "## 🎓 Try it Yourself!\n",
    "\n",
    "Here's the complete workflow to see deployments in action:\n",
    "\n",
    "### 1️⃣ Create the deployment (in a terminal):\n",
    "```bash\n",
    "cd /Users/jichong/projects/playground/BOS/prefect-poc\n",
    "export PREFECT_API_URL=http://localhost:4200/api\n",
    "uv run prefect deploy deployed_flow.py:data_pipeline --name my-data-pipeline --pool my-local-pool\n",
    "```\n",
    "\n",
    "### 2️⃣ Start a worker (in another terminal):\n",
    "```bash\n",
    "cd /Users/jichong/projects/playground/BOS/prefect-poc\n",
    "export PREFECT_API_URL=http://localhost:4200/api\n",
    "uv run prefect worker start --pool my-local-pool\n",
    "```\n",
    "\n",
    "### 3️⃣ Trigger a flow run (in a third terminal or from the UI):\n",
    "```bash\n",
    "# Via CLI:\n",
    "uv run prefect deployment run data-pipeline/my-data-pipeline\n",
    "\n",
    "# Or open http://localhost:4200/deployments and click \"Run\"\n",
    "```\n",
    "\n",
    "### 4️⃣ Watch it execute!\n",
    "- The worker terminal will show the flow executing\n",
    "- The UI at http://localhost:4200 will show real-time progress\n",
    "- All logs and results are tracked by the server\n",
    "\n",
    "---\n",
    "\n",
    "### What you've learned:\n",
    "\n",
    "✅ **Direct execution** (`python flow.py`):\n",
    "- Code runs on your machine\n",
    "- You trigger it manually\n",
    "- Good for development/testing\n",
    "\n",
    "✅ **Deployments + Workers**:\n",
    "- Server orchestrates the execution\n",
    "- Workers pull jobs from work pools\n",
    "- Can schedule flows, handle retries, distribute work\n",
    "- Production-ready approach\n",
    "\n",
    "This is how Prefect powers production data pipelines at scale! 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e30f6b",
   "metadata": {},
   "source": [
    "# Part 3: Blocks and Remote Storage\n",
    "\n",
    "## What are Blocks?\n",
    "\n",
    "**Blocks** are Prefect's way of storing reusable configuration and credentials. Think of them as secure configuration objects that can be:\n",
    "- Created once and reused across many flows\n",
    "- Stored on the Prefect server\n",
    "- Referenced by name in deployments\n",
    "- Used to connect to external systems (databases, cloud storage, git repos, etc.)\n",
    "\n",
    "### Common Block Types:\n",
    "- **Git Repository Blocks**: Connect to GitHub, GitLab, Bitbucket\n",
    "- **Storage Blocks**: S3, GCS, Azure Blob Storage\n",
    "- **Secret Blocks**: Store passwords, API keys\n",
    "- **Infrastructure Blocks**: Docker, Kubernetes, cloud compute\n",
    "\n",
    "### Why Blocks Matter for Private Bitbucket:\n",
    "When your code is in a private Git repository:\n",
    "1. **Worker needs access** to pull the code before running it\n",
    "2. **Credentials must be secure** (not hardcoded in code)\n",
    "3. **Blocks solve both problems** by storing Git credentials on the server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9080f243",
   "metadata": {},
   "source": [
    "## Step 1: Create a Bitbucket Repository Block\n",
    "\n",
    "Let's create a block that stores credentials for your private Bitbucket server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af9ce13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Bitbucket repository block programmatically\n",
    "\n",
    "from prefect.blocks.system import Secret\n",
    "from prefect_bitbucket.credentials import BitBucketCredentials\n",
    "from prefect_bitbucket.repository import BitBucketRepository\n",
    "\n",
    "async def create_bitbucket_block():\n",
    "    \"\"\"\n",
    "    Create a Bitbucket repository block for private repo access.\n",
    "    This block will be stored on the Prefect server.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Create a secret block for the password/token\n",
    "    # (In production, you'd set this more securely)\n",
    "    try:\n",
    "        bitbucket_token = Secret(value=\"your-app-password-or-token\")\n",
    "        await bitbucket_token.save(name=\"bitbucket-token\", overwrite=True)\n",
    "        print(\"✅ Created secret block: bitbucket-token\")\n",
    "    except Exception as e:\n",
    "        print(f\"Note: {e}\")\n",
    "    \n",
    "    # Step 2: Create credentials block\n",
    "    try:\n",
    "        credentials = BitBucketCredentials(\n",
    "            username=\"your-username\",\n",
    "            token=bitbucket_token  # Reference to the secret block\n",
    "        )\n",
    "        await credentials.save(name=\"my-bitbucket-creds\", overwrite=True)\n",
    "        print(\"✅ Created credentials block: my-bitbucket-creds\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  BitBucket block type not installed: {e}\")\n",
    "        print(\"   You would need to: pip install prefect-bitbucket\")\n",
    "    \n",
    "    # Step 3: Create repository block\n",
    "    try:\n",
    "        repo_block = BitBucketRepository(\n",
    "            repository=\"https://bitbucket.mycompany.com/scm/project/repo.git\",\n",
    "            credentials=credentials,\n",
    "            reference=\"main\"  # branch name\n",
    "        )\n",
    "        await repo_block.save(name=\"my-private-repo\", overwrite=True)\n",
    "        print(\"✅ Created repository block: my-private-repo\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Could not create repo block: {e}\")\n",
    "    \n",
    "    print(\"\\n📝 Block created and stored on Prefect server!\")\n",
    "    print(\"   Workers can now use this block to pull code from private Bitbucket\")\n",
    "\n",
    "# Note: This will fail without prefect-bitbucket installed\n",
    "# Showing the pattern for demonstration\n",
    "print(\"Example: Creating Bitbucket Block\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n⚠️  Note: This requires 'prefect-bitbucket' to be installed:\")\n",
    "print(\"   pip install prefect-bitbucket\")\n",
    "print(\"\\nFor demonstration purposes, let's see what blocks we currently have:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79dfd69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Available Block Types on Server:\n",
      "   Found 37 block types\n",
      "\n",
      "   • AWS Credentials\n",
      "     Block used to manage authentication with AWS. AWS authentication is\n",
      "handled via ...\n",
      "   • Azure\n",
      "     Store data as a file on Azure Datalake and Azure Blob Storage....\n",
      "   • Azure Blob Storage Credentials\n",
      "     Block used to manage Blob Storage authentication with Azure.\n",
      "Azure authenticatio...\n",
      "   • Azure Container Instance Credentials\n",
      "     Block used to manage Azure Container Instances authentication. Stores Azure Serv...\n",
      "   • Azure Container Instance Job\n",
      "     Run tasks using Azure Container Instances. Note this block is experimental. The ...\n",
      "   • Azure Cosmos DB Credentials\n",
      "     Block used to manage Cosmos DB authentication with Azure.\n",
      "Azure authentication i...\n",
      "   • AzureML Credentials\n",
      "     Block used to manage authentication with AzureML. Azure authentication is\n",
      "handle...\n",
      "   • BigQuery Warehouse\n",
      "     A block for querying a database with BigQuery.\n",
      "\n",
      "Upon instantiating, a connection...\n",
      "   • Custom Webhook\n",
      "     Enables sending notifications via any custom webhook.\n",
      "\n",
      "All nested string param c...\n",
      "   • Databricks Credentials\n",
      "     Block used to manage Databricks authentication....\n",
      "\n",
      "💡 These blocks can be used in deployments to store credentials and config\n"
     ]
    }
   ],
   "source": [
    "# List all available blocks on the server\n",
    "async def list_blocks():\n",
    "    \"\"\"Query all blocks stored on the Prefect server\"\"\"\n",
    "    async with PrefectClient(api=\"http://localhost:4200/api\") as client:\n",
    "        # Get block types\n",
    "        block_types = await client.read_block_types()\n",
    "        \n",
    "        print(f\"📦 Available Block Types on Server:\")\n",
    "        print(f\"   Found {len(block_types)} block types\\n\")\n",
    "        \n",
    "        for bt in block_types[:10]:  # Show first 10\n",
    "            print(f\"   • {bt.name}\")\n",
    "            if bt.description:\n",
    "                print(f\"     {bt.description[:80]}...\")\n",
    "        \n",
    "        return block_types\n",
    "\n",
    "blocks = await list_blocks()\n",
    "print(f\"\\n💡 These blocks can be used in deployments to store credentials and config\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff52e4f1",
   "metadata": {},
   "source": [
    "## Step 2: How Deployments Work with Private Bitbucket\n",
    "\n",
    "Here's the complete architecture when using a private Bitbucket server:\n",
    "\n",
    "### The Flow:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│  BITBUCKET SERVER (Private)                             │\n",
    "│  https://bitbucket.mycompany.com                        │\n",
    "│                                                         │\n",
    "│  📁 Your Repository:                                    │\n",
    "│    • flow.py (your flow code)                          │\n",
    "│    • requirements.txt                                   │\n",
    "│    • prefect.yaml (deployment config)                  │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "                    ▲\n",
    "                    │ 1. Git clone with credentials\n",
    "                    │\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│  PREFECT SERVER (Your deployment)                       │\n",
    "│  Could be: Docker, cloud, or on-premises                │\n",
    "│                                                         │\n",
    "│  Stores:                                                │\n",
    "│    • 🔐 Bitbucket credentials (Block)                  │\n",
    "│    • 📋 Deployment definitions                         │\n",
    "│    • 🎯 Work pool configurations                       │\n",
    "│    • 📊 Flow run metadata                              │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "                    │\n",
    "                    │ 2. Worker polls for work\n",
    "                    ▼\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│  WORKER (Running on your infrastructure)                │\n",
    "│                                                         │\n",
    "│  When a flow run is scheduled:                          │\n",
    "│    1. Worker gets job from work pool                   │\n",
    "│    2. Worker loads Bitbucket credentials from block     │\n",
    "│    3. Worker clones code from private repo              │\n",
    "│    4. Worker creates Python environment                 │\n",
    "│    5. Worker installs dependencies                      │\n",
    "│    6. Worker executes the flow                          │\n",
    "│    7. Worker sends results back to server               │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Key Points:\n",
    "\n",
    "1. **Prefect server doesn't execute code** - It only stores metadata\n",
    "2. **Worker pulls code** - Worker machine needs network access to Bitbucket\n",
    "3. **Credentials stored securely** - Blocks keep secrets on the server\n",
    "4. **Code is ephemeral** - Worker clones fresh code for each run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3e8226",
   "metadata": {},
   "source": [
    "## Step 3: Creating a Deployment with Bitbucket Storage\n",
    "\n",
    "Here's how to deploy a flow that lives in a private Bitbucket repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9d0ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example deployment configuration with Bitbucket storage\n",
    "\n",
    "deployment_example = \"\"\"\n",
    "# prefect.yaml - Deployment configuration file in your Bitbucket repo\n",
    "\n",
    "name: production-deployment\n",
    "prefect-version: 2.13.7\n",
    "\n",
    "# Define where the code comes from\n",
    "pull:\n",
    "  - prefect.deployments.steps.git_clone:\n",
    "      repository: https://bitbucket.mycompany.com/scm/project/myflows.git\n",
    "      branch: main\n",
    "      credentials: \"{{ prefect.blocks.bitbucket-credentials.my-bitbucket-creds }}\"\n",
    "\n",
    "# Define deployments\n",
    "deployments:\n",
    "  - name: prod-data-pipeline\n",
    "    entrypoint: flows/data_pipeline.py:data_pipeline\n",
    "    work_pool:\n",
    "      name: prod-worker-pool\n",
    "    schedule:\n",
    "      cron: \"0 2 * * *\"  # Run at 2 AM daily\n",
    "    parameters:\n",
    "      env: production\n",
    "\"\"\"\n",
    "\n",
    "print(\"📄 Example prefect.yaml with Bitbucket Storage\")\n",
    "print(\"=\" * 70)\n",
    "print(deployment_example)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"How to deploy from your Bitbucket repository:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "deployment_steps = \"\"\"\n",
    "1️⃣ Push your code to Bitbucket:\n",
    "   • flows/data_pipeline.py (your flow code)\n",
    "   • prefect.yaml (deployment config - shown above)\n",
    "   • requirements.txt (Python dependencies)\n",
    "\n",
    "2️⃣ Create the Bitbucket credentials block (on Prefect server):\n",
    "   uv run prefect block register -m prefect_bitbucket\n",
    "   # Then create the block via UI or Python as shown earlier\n",
    "\n",
    "3️⃣ Deploy from your local machine (one time):\n",
    "   cd /path/to/your/repo\n",
    "   export PREFECT_API_URL=http://your-prefect-server:4200/api\n",
    "   uv run prefect deploy --all\n",
    "\n",
    "   This tells the server about your deployment and where to find the code.\n",
    "\n",
    "4️⃣ Start workers (on your infrastructure):\n",
    "   # These can run anywhere with access to Bitbucket\n",
    "   export PREFECT_API_URL=http://your-prefect-server:4200/api\n",
    "   uv run prefect worker start --pool prod-worker-pool\n",
    "\n",
    "5️⃣ When a flow run is triggered:\n",
    "   • Worker gets the job from the server\n",
    "   • Worker uses the Bitbucket block to clone the repo\n",
    "   • Worker runs the flow from the cloned code\n",
    "   • All logs go back to the server\n",
    "\"\"\"\n",
    "\n",
    "print(deployment_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd9cf0a",
   "metadata": {},
   "source": [
    "## Step 4: Answering Your Questions\n",
    "\n",
    "### Q: \"If I deploy my own Prefect server, how will it connect to the private Bitbucket server?\"\n",
    "\n",
    "**Answer: The Prefect server DOESN'T connect to Bitbucket. The WORKER does!**\n",
    "\n",
    "Here's the breakdown:\n",
    "\n",
    "#### What the Prefect Server Does:\n",
    "- ✅ Stores the Bitbucket credentials (in a Block)\n",
    "- ✅ Stores deployment metadata (which repo, which branch)\n",
    "- ✅ Manages the work queue\n",
    "- ❌ **Does NOT** clone code\n",
    "- ❌ **Does NOT** execute flows\n",
    "- ❌ **Does NOT** need network access to Bitbucket\n",
    "\n",
    "#### What the Worker Does:\n",
    "- ✅ Runs on your infrastructure (can be same server, different server, laptop, cloud VM)\n",
    "- ✅ **NEEDS** network access to your private Bitbucket server\n",
    "- ✅ Pulls credentials from the Prefect server (via API)\n",
    "- ✅ Uses credentials to clone code from Bitbucket\n",
    "- ✅ Executes the flow\n",
    "- ✅ Sends results back to Prefect server\n",
    "\n",
    "### Network Requirements:\n",
    "\n",
    "```\n",
    "┌──────────────────────────────────────────────────┐\n",
    "│  Your Network / VPN                              │\n",
    "│                                                  │\n",
    "│  ┌────────────────┐     ┌──────────────────┐   │\n",
    "│  │ Prefect Server │     │ Bitbucket Server │   │\n",
    "│  │ :4200          │     │ (Private)        │   │\n",
    "│  └────────────────┘     └──────────────────┘   │\n",
    "│         ▲                        ▲              │\n",
    "│         │                        │              │\n",
    "│         │  API calls             │ Git clone    │\n",
    "│         │  (get credentials,     │ (needs auth) │\n",
    "│         │   report status)       │              │\n",
    "│         │                        │              │\n",
    "│  ┌──────┴────────────────────────┴──────┐      │\n",
    "│  │           WORKER                      │      │\n",
    "│  │  • Needs access to BOTH servers       │      │\n",
    "│  │  • Can be same or different machine   │      │\n",
    "│  └───────────────────────────────────────┘      │\n",
    "└──────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Security Best Practices:\n",
    "\n",
    "1. **Worker Placement**: Deploy workers in the same network zone as Bitbucket\n",
    "2. **Credentials**: Use SSH keys or app passwords, not your personal password\n",
    "3. **Least Privilege**: Give the Bitbucket credentials read-only access\n",
    "4. **Network Policies**: Workers need HTTPS/SSH to Bitbucket, HTTPS to Prefect server\n",
    "5. **Secrets**: Never commit credentials to git - use Blocks!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f238a7",
   "metadata": {},
   "source": [
    "## Real-World Example: Production Setup\n",
    "\n",
    "Let's walk through a complete production setup:\n",
    "\n",
    "### Scenario:\n",
    "- Company has private Bitbucket server at `bitbucket.company.com`\n",
    "- Data engineering team writes flows in `data-team/etl-flows` repo\n",
    "- Prefect server deployed on `prefect.company.com`\n",
    "- Workers run on Kubernetes in the same VPC\n",
    "\n",
    "### Setup Steps:\n",
    "\n",
    "#### 1. Install Prefect Bitbucket Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33c6dc41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PRODUCTION SETUP: Prefect + Private Bitbucket\n",
      "================================================================================\n",
      "\n",
      "STEP 1: Install Prefect Bitbucket Integration\n",
      "----------------------------------------------\n",
      "On your development machine and in worker Docker images:\n",
      "\n",
      "    pip install prefect-bitbucket\n",
      "\n",
      "This provides:\n",
      "  • BitBucketCredentials block\n",
      "  • BitBucketRepository block\n",
      "  • Git clone steps for deployments\n",
      "\n",
      "\n",
      "STEP 2: Create Bitbucket App Password\n",
      "--------------------------------------\n",
      "On bitbucket.company.com:\n",
      "  1. Go to Personal Settings → App Passwords\n",
      "  2. Create new app password with 'Repository Read' permission\n",
      "  3. Save the password securely (you'll need it once)\n",
      "\n",
      "\n",
      "STEP 3: Register Block Types & Create Blocks\n",
      "---------------------------------------------\n",
      "# Register the block types with your Prefect server\n",
      "export PREFECT_API_URL=https://prefect.company.com/api\n",
      "prefect block register -m prefect_bitbucket\n",
      "\n",
      "# Create credentials block (via Python or UI)\n",
      "from prefect_bitbucket.credentials import BitBucketCredentials\n",
      "from prefect.blocks.system import Secret\n",
      "\n",
      "async def setup_blocks():\n",
      "    # Create secret for the app password\n",
      "    token = Secret(value=\"your-app-password-here\")\n",
      "    await token.save(\"bitbucket-token\")\n",
      "    \n",
      "    # Create credentials\n",
      "    creds = BitBucketCredentials(\n",
      "        username=\"serviceaccount@company.com\",\n",
      "        token=token\n",
      "    )\n",
      "    await creds.save(\"company-bitbucket-creds\")\n",
      "\n",
      "# Or use the Prefect UI:\n",
      "# Navigate to Blocks → + → BitBucketCredentials\n",
      "\n",
      "\n",
      "STEP 4: Structure Your Bitbucket Repository\n",
      "--------------------------------------------\n",
      "data-team/etl-flows/\n",
      "├── flows/\n",
      "│   ├── daily_etl.py\n",
      "│   ├── weekly_report.py\n",
      "│   └── __init__.py\n",
      "├── requirements.txt\n",
      "└── prefect.yaml\n",
      "\n",
      "# requirements.txt\n",
      "prefect==2.13.7\n",
      "pandas\n",
      "sqlalchemy\n",
      "psycopg2-binary\n",
      "\n",
      "# prefect.yaml\n",
      "name: etl-flows\n",
      "prefect-version: 2.13.7\n",
      "\n",
      "pull:\n",
      "  - prefect.deployments.steps.git_clone:\n",
      "      repository: https://bitbucket.company.com/scm/data-team/etl-flows.git\n",
      "      branch: \"{{ branch }}\"\n",
      "      credentials: \"{{ prefect.blocks.bitbucket-credentials.company-bitbucket-creds }}\"\n",
      "\n",
      "deployments:\n",
      "  - name: daily-etl-prod\n",
      "    entrypoint: flows/daily_etl.py:main_flow\n",
      "    work_pool:\n",
      "      name: k8s-prod-pool\n",
      "    schedule:\n",
      "      cron: \"0 1 * * *\"\n",
      "    parameters:\n",
      "      environment: production\n",
      "\n",
      "\n",
      "STEP 5: Deploy from Your Machine (One Time)\n",
      "--------------------------------------------\n",
      "cd /path/to/etl-flows\n",
      "export PREFECT_API_URL=https://prefect.company.com/api\n",
      "prefect deploy --all\n",
      "\n",
      "This registers the deployment with the server. The server now knows:\n",
      "  • Where to find the code (Bitbucket URL)\n",
      "  • Which credentials to use (the block name)\n",
      "  • Which branch to use (main)\n",
      "  • Schedule and parameters\n",
      "\n",
      "\n",
      "STEP 6: Run Workers (Continuously)\n",
      "-----------------------------------\n",
      "Workers run on your infrastructure (K8s, VMs, etc)\n",
      "\n",
      "# Kubernetes deployment.yaml\n",
      "apiVersion: apps/v1\n",
      "kind: Deployment\n",
      "metadata:\n",
      "  name: prefect-worker\n",
      "spec:\n",
      "  replicas: 3\n",
      "  template:\n",
      "    spec:\n",
      "      containers:\n",
      "      - name: worker\n",
      "        image: prefecthq/prefect:2.13.7-python3.10\n",
      "        command: [\"prefect\", \"worker\", \"start\"]\n",
      "        args: [\"--pool\", \"k8s-prod-pool\"]\n",
      "        env:\n",
      "        - name: PREFECT_API_URL\n",
      "          value: \"https://prefect.company.com/api\"\n",
      "\n",
      "Workers will:\n",
      "  1. Poll the work pool for jobs\n",
      "  2. When a job arrives, load credentials from the server\n",
      "  3. Clone code from Bitbucket using those credentials\n",
      "  4. Execute the flow\n",
      "  5. Report results back\n",
      "\n",
      "\n",
      "STEP 7: Trigger & Monitor\n",
      "--------------------------\n",
      "Flows run automatically on schedule, or trigger manually:\n",
      "\n",
      "  prefect deployment run daily-etl-prod/daily-etl-prod\n",
      "\n",
      "Monitor at: https://prefect.company.com/deployments\n",
      "\n",
      "\n",
      "SECURITY NOTES:\n",
      "--------------\n",
      "✅ Credentials never in code - stored in Blocks on server\n",
      "✅ Workers pull fresh code each run - no stale code\n",
      "✅ Use service account with read-only Bitbucket access\n",
      "✅ Workers need network access to both Prefect server AND Bitbucket\n",
      "✅ Prefect server only needs to be accessible to workers (not Bitbucket)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Complete production example walkthrough\n",
    "\n",
    "production_guide = \"\"\"\n",
    "================================================================================\n",
    "PRODUCTION SETUP: Prefect + Private Bitbucket\n",
    "================================================================================\n",
    "\n",
    "STEP 1: Install Prefect Bitbucket Integration\n",
    "----------------------------------------------\n",
    "On your development machine and in worker Docker images:\n",
    "\n",
    "    pip install prefect-bitbucket\n",
    "\n",
    "This provides:\n",
    "  • BitBucketCredentials block\n",
    "  • BitBucketRepository block\n",
    "  • Git clone steps for deployments\n",
    "\n",
    "\n",
    "STEP 2: Create Bitbucket App Password\n",
    "--------------------------------------\n",
    "On bitbucket.company.com:\n",
    "  1. Go to Personal Settings → App Passwords\n",
    "  2. Create new app password with 'Repository Read' permission\n",
    "  3. Save the password securely (you'll need it once)\n",
    "\n",
    "\n",
    "STEP 3: Register Block Types & Create Blocks\n",
    "---------------------------------------------\n",
    "# Register the block types with your Prefect server\n",
    "export PREFECT_API_URL=https://prefect.company.com/api\n",
    "prefect block register -m prefect_bitbucket\n",
    "\n",
    "# Create credentials block (via Python or UI)\n",
    "from prefect_bitbucket.credentials import BitBucketCredentials\n",
    "from prefect.blocks.system import Secret\n",
    "\n",
    "async def setup_blocks():\n",
    "    # Create secret for the app password\n",
    "    token = Secret(value=\"your-app-password-here\")\n",
    "    await token.save(\"bitbucket-token\")\n",
    "    \n",
    "    # Create credentials\n",
    "    creds = BitBucketCredentials(\n",
    "        username=\"serviceaccount@company.com\",\n",
    "        token=token\n",
    "    )\n",
    "    await creds.save(\"company-bitbucket-creds\")\n",
    "\n",
    "# Or use the Prefect UI:\n",
    "# Navigate to Blocks → + → BitBucketCredentials\n",
    "\n",
    "\n",
    "STEP 4: Structure Your Bitbucket Repository\n",
    "--------------------------------------------\n",
    "data-team/etl-flows/\n",
    "├── flows/\n",
    "│   ├── daily_etl.py\n",
    "│   ├── weekly_report.py\n",
    "│   └── __init__.py\n",
    "├── requirements.txt\n",
    "└── prefect.yaml\n",
    "\n",
    "# requirements.txt\n",
    "prefect==2.13.7\n",
    "pandas\n",
    "sqlalchemy\n",
    "psycopg2-binary\n",
    "\n",
    "# prefect.yaml\n",
    "name: etl-flows\n",
    "prefect-version: 2.13.7\n",
    "\n",
    "pull:\n",
    "  - prefect.deployments.steps.git_clone:\n",
    "      repository: https://bitbucket.company.com/scm/data-team/etl-flows.git\n",
    "      branch: \"{{ branch }}\"\n",
    "      credentials: \"{{ prefect.blocks.bitbucket-credentials.company-bitbucket-creds }}\"\n",
    "\n",
    "deployments:\n",
    "  - name: daily-etl-prod\n",
    "    entrypoint: flows/daily_etl.py:main_flow\n",
    "    work_pool:\n",
    "      name: k8s-prod-pool\n",
    "    schedule:\n",
    "      cron: \"0 1 * * *\"\n",
    "    parameters:\n",
    "      environment: production\n",
    "\n",
    "\n",
    "STEP 5: Deploy from Your Machine (One Time)\n",
    "--------------------------------------------\n",
    "cd /path/to/etl-flows\n",
    "export PREFECT_API_URL=https://prefect.company.com/api\n",
    "prefect deploy --all\n",
    "\n",
    "This registers the deployment with the server. The server now knows:\n",
    "  • Where to find the code (Bitbucket URL)\n",
    "  • Which credentials to use (the block name)\n",
    "  • Which branch to use (main)\n",
    "  • Schedule and parameters\n",
    "\n",
    "\n",
    "STEP 6: Run Workers (Continuously)\n",
    "-----------------------------------\n",
    "Workers run on your infrastructure (K8s, VMs, etc)\n",
    "\n",
    "# Kubernetes deployment.yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: prefect-worker\n",
    "spec:\n",
    "  replicas: 3\n",
    "  template:\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: worker\n",
    "        image: prefecthq/prefect:2.13.7-python3.10\n",
    "        command: [\"prefect\", \"worker\", \"start\"]\n",
    "        args: [\"--pool\", \"k8s-prod-pool\"]\n",
    "        env:\n",
    "        - name: PREFECT_API_URL\n",
    "          value: \"https://prefect.company.com/api\"\n",
    "\n",
    "Workers will:\n",
    "  1. Poll the work pool for jobs\n",
    "  2. When a job arrives, load credentials from the server\n",
    "  3. Clone code from Bitbucket using those credentials\n",
    "  4. Execute the flow\n",
    "  5. Report results back\n",
    "\n",
    "\n",
    "STEP 7: Trigger & Monitor\n",
    "--------------------------\n",
    "Flows run automatically on schedule, or trigger manually:\n",
    "\n",
    "  prefect deployment run daily-etl-prod/daily-etl-prod\n",
    "\n",
    "Monitor at: https://prefect.company.com/deployments\n",
    "\n",
    "\n",
    "SECURITY NOTES:\n",
    "--------------\n",
    "✅ Credentials never in code - stored in Blocks on server\n",
    "✅ Workers pull fresh code each run - no stale code\n",
    "✅ Use service account with read-only Bitbucket access\n",
    "✅ Workers need network access to both Prefect server AND Bitbucket\n",
    "✅ Prefect server only needs to be accessible to workers (not Bitbucket)\n",
    "\"\"\"\n",
    "\n",
    "print(production_guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2616de",
   "metadata": {},
   "source": [
    "## Summary: Key Takeaways\n",
    "\n",
    "### 🔑 Blocks Explained:\n",
    "- **What**: Reusable configuration objects stored on Prefect server\n",
    "- **Why**: Securely store credentials, avoid hardcoding secrets\n",
    "- **How**: Create once, reference by name in deployments\n",
    "- **Types**: Git repos, databases, cloud storage, secrets, infrastructure\n",
    "\n",
    "### 🔒 Private Bitbucket + Prefect Architecture:\n",
    "\n",
    "| Component | Role | Needs Access To |\n",
    "|-----------|------|-----------------|\n",
    "| **Bitbucket Server** | Stores your flow code | Nothing (it's the source) |\n",
    "| **Prefect Server** | Stores metadata, credentials, schedules | Nothing (workers connect to it) |\n",
    "| **Worker** | Executes flows | **Both** Bitbucket AND Prefect Server |\n",
    "\n",
    "### 📊 The Complete Flow:\n",
    "\n",
    "1. **Developer** pushes code to private Bitbucket\n",
    "2. **Developer** creates Bitbucket credentials Block on Prefect server\n",
    "3. **Developer** runs `prefect deploy` to register deployment\n",
    "4. **Prefect Server** stores deployment metadata (not the code!)\n",
    "5. **Worker** polls Prefect server for work\n",
    "6. **Worker** gets credentials from Prefect server (via Block)\n",
    "7. **Worker** clones code from Bitbucket using those credentials\n",
    "8. **Worker** executes the flow\n",
    "9. **Worker** sends results back to Prefect server\n",
    "10. **You** view results in Prefect UI\n",
    "\n",
    "### ✅ What You've Learned:\n",
    "\n",
    "1. **Blocks** are Prefect's way to store reusable config and credentials\n",
    "2. **Prefect server** never executes code or clones repos\n",
    "3. **Workers** do all the heavy lifting (clone + execute)\n",
    "4. **Network access**: Workers need access to both Prefect server and Bitbucket\n",
    "5. **Security**: Credentials in Blocks, never in code\n",
    "6. **Deployment**: Code stays in git, workers pull it fresh each time\n",
    "\n",
    "### 🚀 Next Steps:\n",
    "\n",
    "1. Install `prefect-bitbucket`: `pip install prefect-bitbucket`\n",
    "2. Create a service account in Bitbucket with read-only access\n",
    "3. Create Bitbucket credentials Block on your Prefect server\n",
    "4. Add `prefect.yaml` to your repository\n",
    "5. Deploy: `prefect deploy --all`\n",
    "6. Start workers in your infrastructure\n",
    "7. Watch flows run from your private Bitbucket repo!\n",
    "\n",
    "---\n",
    "\n",
    "**Remember**: The beauty of this architecture is that your Prefect server can be anywhere (cloud, on-prem, DMZ), and your workers just need network access to both the server and Bitbucket. The server itself never needs to reach Bitbucket! 🎯"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prefect-poc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
